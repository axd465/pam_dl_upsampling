{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Pre-Start Procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lSu55NMAg340"
   },
   "source": [
    "## Import Statements (Always Run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "phIROnd7F_JQ",
    "outputId": "7c4d194e-0170-49ee-9932-05c46eacbe94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.1.0\n"
     ]
    }
   ],
   "source": [
    "# customary imports:\n",
    "import tensorflow as tf\n",
    "assert '2.' in tf.__version__  # make sure you're using tf 2.0\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math, string, random, datetime\n",
    "import os, shutil, h5py, copy, re\n",
    "import requests, zipfile, io\n",
    "import skimage\n",
    "import scipy\n",
    "from skimage import exposure\n",
    "import seaborn as sns\n",
    "import PIL\n",
    "import imageio\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    # Load the Drive helper and mount\n",
    "    from google.colab import drive\n",
    "    # This will prompt for authorization.\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "else:\n",
    "    import utils.preprocess_crop\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import Model\n",
    "print('Tensorflow version: ' + tf.__version__)\n",
    "if tf.__version__ == '2.2.0':\n",
    "    import tensorflow_addons as tfa\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lcyGPWatiZRx"
   },
   "source": [
    "## *If Running In Colab* - Sets Current Working Dir to Your Google Drive Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "JzTqxwZUg346",
    "outputId": "85eee941-d05e-48ce-c77e-4882da6a76a5"
   },
   "outputs": [],
   "source": [
    "your_drive_dir = 'YourGoogleDriveDirectoryPath'\n",
    "os.chdir('/content/drive/' + your_drive_dir)\n",
    "print(\"Current Working Directory is : \" + os.getcwd())\n",
    "import utils.preprocess_crop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and Seperate Data into Train/Val/Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Download the Vessel Dataset From Online (Preprocessing is already done):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'combined_vessel_data'\n",
    "ZIP_FILE_URL = 'https://ndownloader.figshare.com/articles/12871805/versions/1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Downloading Files...')\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.mkdir(DATA_DIR)\n",
    "    r = requests.get(ZIP_FILE_URL)\n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    z.extractall(f'./{DATA_DIR}')\n",
    "print('Files Downloaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YecWDY3Bg35F",
    "toc-hr-collapsed": false
   },
   "source": [
    "## Function to Seperate Folder into Subfolder of /training /validation and /testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sJ3bvzrRg35G"
   },
   "outputs": [],
   "source": [
    "# Import Statements:\n",
    "from utils.data_preprocessing_utils import data_seperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9DBhIZspg35I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Data Seperation...\n",
      "Ending Data Seperation\n"
     ]
    }
   ],
   "source": [
    "input_dir = DATA_DIR\n",
    "output_dir = 'data'\n",
    "delete_previous = True\n",
    "file_format = '.tif'\n",
    "dataset_percentages = (80, 10, 10)\n",
    "\n",
    "print('Starting Data Seperation...')\n",
    "train_dir, val_dir, test_dir = data_seperation(input_dir, output_dir, dataset_percentages, \n",
    "                                               delete_previous, file_format)\n",
    "print('Ending Data Seperation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NmQuhivrg35K"
   },
   "source": [
    "### Code Block to Show How Much of Data has been Placed in various Subfolders (train/val/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y9jHWHGXg35K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training directory = 304\n",
      "Length of validation directory = 39\n",
      "Length of test directory = 38\n"
     ]
    }
   ],
   "source": [
    "# print('Length of raw_data directory = '+str(len(os.listdir('raw_data'))))\n",
    "# print('Length of converted_data directory = '+str(len(os.listdir('converted_data'))))\n",
    "# print('Rejected Files = ' + str(len(os.listdir('raw_data')) - len(os.listdir('converted_data'))))\n",
    "print('Length of training directory = '+str(len(os.listdir(train_dir))))\n",
    "print('Length of validation directory = '+str(len(os.listdir(val_dir))))\n",
    "print('Length of test directory = '+str(len(os.listdir(test_dir))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o2GmsjLvg35M",
    "toc-hr-collapsed": true
   },
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7fo4CI9ng35V"
   },
   "source": [
    "## Padding Training Data and Adding Downsampled Image as Channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Statements:\n",
    "from utils.standardize_dir_utils import pad_img_and_add_down_channel\n",
    "from utils.standardize_dir_utils import standardize_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir = 'data'\n",
    "STANDARD_IMAGE_SHAPE = (128,128,1)\n",
    "downsample_axis = 'both'\n",
    "downsample_ratio = [1,5]\n",
    "file_format = '.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hh78mm5_g35Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_train_image_pool = 304\n"
     ]
    }
   ],
   "source": [
    "input_dir = main_dir + '/train/input'\n",
    "total_train_image_pool = standardize_dir(input_dir, downsample_axis, downsample_ratio, \n",
    "                                         STANDARD_IMAGE_SHAPE, file_format)\n",
    "print('total_train_image_pool = ' + str(total_train_image_pool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dcU9kOvTg35d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_val_image_pool = 39\n"
     ]
    }
   ],
   "source": [
    "input_dir = main_dir + '/val/input'\n",
    "total_val_image_pool = standardize_dir(input_dir, downsample_axis, downsample_ratio, \n",
    "                                       STANDARD_IMAGE_SHAPE, file_format)\n",
    "print('total_val_image_pool = ' + str(total_val_image_pool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gb1xMjYNg35g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_test_image_pool = 38\n"
     ]
    }
   ],
   "source": [
    "input_dir = main_dir + '/test/input'\n",
    "total_test_image_pool = standardize_dir(input_dir, downsample_axis, downsample_ratio, \n",
    "                                        STANDARD_IMAGE_SHAPE, file_format)\n",
    "print('total_test_image_pool = ' + str(total_test_image_pool))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Model Testing and Training - Run After this Point if Restarting Kernel (and you have re-run the import statements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QFIyuzErg35i"
   },
   "source": [
    "## Data Loader and Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_aug_funcs import preprocess_function\n",
    "from utils.data_aug_funcs import preprocess_function_valtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RD_57OwMg35o"
   },
   "outputs": [],
   "source": [
    "STANDARD_IMAGE_SHAPE = (128,128,1) # See Above Section to Verify Shape\n",
    "MIN_SHAPE = (STANDARD_IMAGE_SHAPE[0],STANDARD_IMAGE_SHAPE[1])\n",
    "BATCH_SIZE = 16\n",
    "main_dir = 'data'\n",
    "train_dir = main_dir + '/train'\n",
    "val_dir = main_dir + '/val'\n",
    "test_dir = main_dir + '/test'\n",
    "seed = 7\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen_args_train = dict(horizontal_flip=True,\n",
    "                           vertical_flip = True,\n",
    "                           rotation_range=20,\n",
    "                           width_shift_range=0.1,#0.2\n",
    "                           height_shift_range=0.1,#0.2\n",
    "                           shear_range=0.2,#0.2\n",
    "                           fill_mode='constant',\n",
    "                           cval=0,\n",
    "                           preprocessing_function=preprocess_function,\n",
    "                           dtype='float32')\n",
    "train_datagen = ImageDataGenerator(**data_gen_args_train)\n",
    "data_gen_args_test_and_val = dict(preprocessing_function = preprocess_function_valtest)\n",
    "test_and_val_datagen = ImageDataGenerator(**data_gen_args_test_and_val)\n",
    "# Flow From Directory Generators\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                    target_size=MIN_SHAPE,\n",
    "                                                    batch_size=BATCH_SIZE,\n",
    "                                                    class_mode=None,\n",
    "                                                    ####################\n",
    "                                                    # Monkey Patch - utils.preprocess_crop.py\n",
    "                                                    interpolation = 'lanczos:random',\n",
    "                                                    ####################\n",
    "                                                    color_mode='rgb',\n",
    "                                                    shuffle = True, \n",
    "                                                    seed=seed)\n",
    "\n",
    "validation_generator = test_and_val_datagen.flow_from_directory(val_dir,\n",
    "                                                                target_size=MIN_SHAPE,\n",
    "                                                                batch_size=BATCH_SIZE,\n",
    "                                                                class_mode=None,\n",
    "                                                                ####################\n",
    "                                                                # Monkey Patch - utils.preprocess_crop.py\n",
    "                                                                interpolation = 'lanczos:random',\n",
    "                                                                ####################\n",
    "                                                                color_mode='rgb',\n",
    "                                                                shuffle = True, \n",
    "                                                                seed=seed)\n",
    "test_generator = test_and_val_datagen.flow_from_directory(test_dir,\n",
    "                                                          target_size=MIN_SHAPE,\n",
    "                                                          batch_size=BATCH_SIZE,\n",
    "                                                          class_mode=None,\n",
    "                                                          ####################\n",
    "                                                          # Monkey Patch - utils.preprocess_crop.py\n",
    "                                                          interpolation = 'lanczos:center', \n",
    "                                                          ####################\n",
    "                                                          color_mode='rgb', \n",
    "                                                          shuffle = False,\n",
    "                                                          seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_generator_wrapper(gen):\n",
    "    '''\n",
    "    This wrapper function ensures that the downsampled data and ground truth\n",
    "    stay coaligned during the random crop step.\n",
    "    '''\n",
    "    for data in gen:\n",
    "        orig_img = data[...,0] # target\n",
    "        down_img = data[...,1] # input\n",
    "        # Third channel has mask\n",
    "        yield down_img[..., None], orig_img[..., None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cr_FDSPeg35s",
    "toc-hr-collapsed": false
   },
   "source": [
    "## Model Definition for U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o9z_e6fNg35t"
   },
   "outputs": [],
   "source": [
    "# Various Models:\n",
    "#from models.UNet import getModel\n",
    "#from models.Res_UNet import getModel\n",
    "#from models.ResICL_UNet import getModel\n",
    "#from models.FD_UNet import getModel\n",
    "from models.FD_UNet_SpatialDropout import getModel\n",
    "\n",
    "from utils.model_utils import model_loss\n",
    "from utils.model_utils import PSNR\n",
    "from utils.model_utils import SSIM\n",
    "from utils.model_utils import KLDivergence\n",
    "from utils.model_utils import SavingMetric\n",
    "from utils.model_utils import model_loss_experimental"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters:\n",
    "EPOCHS = 200\n",
    "FILTERS = 64\n",
    "INITIAL_LR = 0.005\n",
    "ACTIVATION = 'elu'       \n",
    "KERNEL_SIZE = 3 # 3x3 kernel\n",
    "AUGMENTATIONS = 10\n",
    "VAL_AUGMENTATIONS = 10\n",
    "USE_MSE = True\n",
    "PROB = 0.05\n",
    "\n",
    "if USE_MSE:\n",
    "    B1 = 1.0\n",
    "    B2 = 0.000075#0.000075\n",
    "    B3 = 0.001#0.001#0.002#0.001\n",
    "else: \n",
    "    # MAE\n",
    "    B1 = 1.0\n",
    "    B2 = 0.0025\n",
    "    B3 = 0.005\n",
    "\n",
    "# Loss functions and Metrics:\n",
    "saving_metric = 'val_loss' # 'val_SavingMetric'\n",
    "saving_metric_2 = 'val_ssim_psnr'\n",
    "loss_func = model_loss_experimental(B1,B2,B3, mse = USE_MSE) # loss=model_loss(B1,B2)\n",
    "fourier_loss = model_loss(B1=0.0, B2=1.0, name='fourier_loss')\n",
    "ssim_psnr = model_loss_experimental(B1=0, B2=0, B3=1.0, mse = USE_MSE, name='ssim_psnr')\n",
    "\n",
    "# Directories and Dir Protecols:\n",
    "train_dir = main_dir + '/train/input'\n",
    "val_dir = main_dir + '/val/input'\n",
    "model_dir = 'model_dir'\n",
    "model_dir_ssim_psnr = 'model_dir_ssim_psnr'\n",
    "delete_previous = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling and Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functional API Model\n",
    "if getModel.__name__ not in ['FD_UNet_Model', 'FD_UNet_Model_Spatial_Dropout', 'ResICL_UNet_Model']:\n",
    "    unet_model = getModel(input_shape=STANDARD_IMAGE_SHAPE, filters=FILTERS, kernel_size=KERNEL_SIZE, activation=ACTIVATION)\n",
    "elif getModel.__name__ is 'FD_UNet_Model':\n",
    "    unet_model = getModel(input_shape=STANDARD_IMAGE_SHAPE, filters=FILTERS//2, kernel_size=KERNEL_SIZE, activation=ACTIVATION)\n",
    "elif getModel.__name__ is 'ResICL_UNet_Model':\n",
    "    unet_model = getModel(input_shape=STANDARD_IMAGE_SHAPE, filters=FILTERS, kernel_size=KERNEL_SIZE, activation=ACTIVATION, prob=PROB)\n",
    "else:\n",
    "    # IF FD-UNET With Spatial Dropout\n",
    "    unet_model = getModel(input_shape=STANDARD_IMAGE_SHAPE, filters=FILTERS//2, kernel_size=KERNEL_SIZE, activation=ACTIVATION, prob=PROB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XUgou4RWg35x"
   },
   "outputs": [],
   "source": [
    "#unet_model = saved_model\n",
    "unet_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=INITIAL_LR, amsgrad=False),  # pick an optimizer\n",
    "                   loss=loss_func,\n",
    "                   metrics=['mean_absolute_error', 'mean_squared_error', \n",
    "                            fourier_loss, PSNR, SSIM, ssim_psnr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y_xdIjQAg351",
    "outputId": "cc88daf4-f8ef-482c-99a9-cc603da1ba4b"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(model_dir):\n",
    "            os.mkdir(model_dir)\n",
    "elif delete_previous:\n",
    "            shutil.rmtree(model_dir)\n",
    "            os.mkdir(model_dir)\n",
    "if not os.path.exists(model_dir_ssim_psnr):\n",
    "            os.mkdir(model_dir_ssim_psnr)\n",
    "elif delete_previous:\n",
    "            shutil.rmtree(model_dir_ssim_psnr)\n",
    "            os.mkdir(model_dir_ssim_psnr)\n",
    "\n",
    "my_callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath = os.path.join(model_dir, 'saved_model.epoch_{epoch:02d}-SSIM_{val_SSIM:.5f}-PSNR_{val_PSNR:.5f}-metric_{'+f'{saving_metric}'+':.5f}.h5'),\n",
    "                                                   monitor=saving_metric,\n",
    "                                                   verbose=0, \n",
    "                                                   save_best_only=True, save_weights_only=False, \n",
    "                                                   mode='min', save_freq='epoch'),\n",
    "                tf.keras.callbacks.ModelCheckpoint(filepath = os.path.join(model_dir_ssim_psnr, 'saved_model.epoch_{epoch:02d}-SSIM_{val_SSIM:.5f}-PSNR_{val_PSNR:.5f}-metric_{'+f'{saving_metric}'+':.5f}.h5'),\n",
    "                                                   monitor=saving_metric_2,\n",
    "                                                   verbose=0, \n",
    "                                                   save_best_only=True, save_weights_only=False, \n",
    "                                                   mode='min', save_freq='epoch')]\n",
    "\n",
    "history = unet_model.fit(custom_generator_wrapper(train_generator),\n",
    "                         steps_per_epoch=AUGMENTATIONS*np.ceil(len(os.listdir(train_dir))/BATCH_SIZE),\n",
    "                         epochs=EPOCHS,\n",
    "                         callbacks=my_callbacks, \n",
    "                         validation_data=custom_generator_wrapper(validation_generator),\n",
    "                         validation_steps=VAL_AUGMENTATIONS*np.ceil(len(os.listdir(val_dir))/BATCH_SIZE), \n",
    "                         max_queue_size=AUGMENTATIONS*np.ceil(len(os.listdir(train_dir))/BATCH_SIZE),\n",
    "                         shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Best Model From Model Dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model_utils import model_loss\n",
    "from utils.model_utils import model_loss_experimental\n",
    "from utils.model_utils import PSNR\n",
    "from utils.model_utils import SSIM\n",
    "from utils.model_utils import KLDivergence\n",
    "from utils.model_utils import SavingMetric\n",
    "print('Loading Best Model...')\n",
    "model_dir = 'model_dir'\n",
    "file_list = [file for file in os.listdir(model_dir) if os.path.isfile(os.path.join(model_dir, file))]\n",
    "filename = sorted(file_list, key = lambda x : int(x.partition('h_')[2].partition('-S')[0]))[-1]\n",
    "directory = os.path.join(os.getcwd(), model_dir)\n",
    "directory = os.path.join(directory, filename)\n",
    "unet_model = tf.keras.models.load_model(directory, compile=False) # To Fine-Tune\n",
    "'''\n",
    "if 'USE_MSE' not in locals() and 'USE_MSE' not globals():\n",
    "    USE_MSE = True\n",
    "fourier_loss = model_loss(B1=0.0, B2=1.0, name='fourier_loss')\n",
    "save_func = model_loss_experimental(B1=0,B2=0,B3=1, mse = USE_MSE, name='save_func')\n",
    "ssim_psnr = model_loss_experimental(B1=0,B2=0,B3=1, mse = USE_MSE, name='ssim_psnr')\n",
    "saved_model = tf.keras.models.load_model(directory, custom_objects={'model_loss':model_loss,'loss_func':model_loss(B1=0.99,B2=0.01), \n",
    "                                                                    'PSNR':PSNR, 'SSIM':SSIM, 'KLDivergence':KLDivergence, \n",
    "                                                                    'SavingMetric':SavingMetric, 'fourier_loss':fourier_loss, \n",
    "                                                                    'save_func':save_func, 'ssim_psnr':ssim_psnr})\n",
    "#'''\n",
    "print('Done Loading Best Model (' + filename + ') from: ' + model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Best Model From Model Dir (SSIM-PSNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model_utils import model_loss\n",
    "from utils.model_utils import model_loss_experimental\n",
    "from utils.model_utils import PSNR\n",
    "from utils.model_utils import SSIM\n",
    "from utils.model_utils import KLDivergence\n",
    "from utils.model_utils import SavingMetric\n",
    "print('Loading Best Model...')\n",
    "model_dir = 'model_dir_ssim_psnr'\n",
    "file_list = [file for file in os.listdir(model_dir) if os.path.isfile(os.path.join(model_dir, file))]\n",
    "filename = sorted(file_list, key = lambda x : int(x.partition('h_')[2].partition('-S')[0]))[-1]\n",
    "directory = os.path.join(os.getcwd(), model_dir)\n",
    "directory = os.path.join(directory, filename)\n",
    "#unet_model = tf.keras.models.load_model(directory, compile=False) # To Fine-Tune\n",
    "#'''\n",
    "if 'USE_MSE' not in locals() and 'USE_MSE' not in globals():\n",
    "    USE_MSE = True\n",
    "fourier_loss = model_loss(B1=0.0, B2=1.0, name='fourier_loss')\n",
    "save_func = model_loss_experimental(B1=0,B2=0,B3=1, mse = USE_MSE, name='save_func')\n",
    "ssim_psnr = model_loss_experimental(B1=0,B2=0,B3=1, mse = USE_MSE, name='ssim_psnr')\n",
    "saved_model = tf.keras.models.load_model(directory, custom_objects={'model_loss':model_loss,'loss_func':model_loss(B1=0.99,B2=0.01), \n",
    "                                                                    'PSNR':PSNR, 'SSIM':SSIM, 'KLDivergence':KLDivergence, \n",
    "                                                                    'SavingMetric':SavingMetric, 'fourier_loss':fourier_loss, \n",
    "                                                                    'save_func':save_func, 'ssim_psnr':ssim_psnr})\n",
    "#'''\n",
    "print('Done Loading Best Model (' + filename + ') from: ' + model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Model Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.history_utils import show_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GLQNY0gug354",
    "outputId": "904f230a-ebf3-4382-83c5-0fdf391fbbfa"
   },
   "outputs": [],
   "source": [
    "offset = 0\n",
    "show_history(history, offset=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5v1l_1X3g358",
    "outputId": "bbc99361-04c0-4bb7-a616-2672a0c56a8a"
   },
   "outputs": [],
   "source": [
    "unet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Show Model Graph:\n",
    "# tf.keras.utils.plot_model(unet_model, 'unet_arch_TB.png', show_shapes=True, rankdir='TB', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.save_model_utils import save_model\n",
    "output_dir = 'saved_model_FD_UNet_5i-10j'\n",
    "model_dir_list = ['model_dir', 'model_dir_ssim_psnr']\n",
    "save_model(unet_model, output_dir, model_dir_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Functional API - Best Model State (can be for Fine-Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model_utils import model_loss\n",
    "from utils.model_utils import model_loss_experimental\n",
    "from utils.model_utils import PSNR\n",
    "from utils.model_utils import SSIM\n",
    "from utils.model_utils import KLDivergence\n",
    "from utils.model_utils import SavingMetric\n",
    "print('Loading Model...')\n",
    "input_dir = 'saved_model_FD_UNet_5i-10j/best_models_from_model_dir_ssim_psnr'\n",
    "USE_MSE = True\n",
    "file_list = [file for file in os.listdir(input_dir) if os.path.isfile(os.path.join(input_dir, file))]\n",
    "filename = sorted(file_list, key = lambda x : int(x.partition('h_')[2].partition('-S')[0]))[-1]\n",
    "directory = os.path.join(os.getcwd(), input_dir)\n",
    "directory = os.path.join(directory, filename)\n",
    "#unet_model = tf.keras.models.load_model(directory, compile=False) # To Fine-Tune\n",
    "#'''\n",
    "fourier_loss = model_loss(B1=0.0, B2=1.0, name='fourier_loss')\n",
    "save_func = model_loss_experimental(B1=0,B2=0,B3=1, mse = USE_MSE, name='save_func')\n",
    "ssim_psnr = model_loss_experimental(B1=0,B2=0,B3=1, mse = USE_MSE, name='ssim_psnr')\n",
    "saved_model = tf.keras.models.load_model(directory, custom_objects={'model_loss':model_loss,'loss_func':model_loss(B1=0.99,B2=0.01), \n",
    "                                                                    'PSNR':PSNR, 'SSIM':SSIM, 'KLDivergence':KLDivergence, \n",
    "                                                                    'SavingMetric':SavingMetric, 'fourier_loss':fourier_loss, \n",
    "                                                                    'save_func':save_func, 'ssim_psnr':ssim_psnr})\n",
    "#''' \n",
    "print('Done Loading Best Model (' + filename + ') from: ' + input_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Functional API - Latest Model State (can be for Fine-Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model_utils import model_loss\n",
    "from utils.model_utils import model_loss_experimental\n",
    "from utils.model_utils import PSNR\n",
    "from utils.model_utils import SSIM\n",
    "from utils.model_utils import KLDivergence\n",
    "from utils.model_utils import SavingMetric\n",
    "print('Loading Model...')\n",
    "input_dir = 'saved_model_FD_UNet_5i-10j'\n",
    "USE_MSE = True\n",
    "filename = 'saved_model.h5'\n",
    "directory = os.path.join(os.getcwd(), input_dir)\n",
    "directory = os.path.join(directory, filename)\n",
    "#unet_model = tf.keras.models.load_model(directory, compile=False) # To Fine-Tune\n",
    "#'''\n",
    "fourier_loss = model_loss(B1=0.0, B2=1.0, name='fourier_loss')\n",
    "save_func = model_loss_experimental(B1=0,B2=0,B3=1, mse = USE_MSE, name='save_func')\n",
    "ssim_psnr = model_loss_experimental(B1=0,B2=0,B3=1, mse = USE_MSE, name='ssim_psnr')\n",
    "saved_model = tf.keras.models.load_model(directory, custom_objects={'model_loss':model_loss,'loss_func':model_loss(B1=0.99,B2=0.01), \n",
    "                                                                    'PSNR':PSNR, 'SSIM':SSIM, 'KLDivergence':KLDivergence, \n",
    "                                                                    'SavingMetric':SavingMetric, 'fourier_loss':fourier_loss, \n",
    "                                                                    'save_func':save_func, 'ssim_psnr':ssim_psnr})\n",
    "#'''                                                                    \n",
    "print('Done Loading Latest Model ' + filename + ' from: ' + input_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load History From File And Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.history_utils import load_history_from_saved\n",
    "from utils.history_utils import show_history\n",
    "\n",
    "input_dir = 'saved_model_FD_UNet_5i-10j'\n",
    "saved_hist = load_history_from_saved(input_dir)\n",
    "show_history(saved_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Showing Results with Test Images (Zero-Fill Method):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model_utils import model_loss\n",
    "from utils.model_utils import model_loss_experimental\n",
    "from utils.model_utils import PSNR\n",
    "from utils.model_utils import SSIM\n",
    "from utils.model_utils import KLDivergence\n",
    "from utils.model_utils import SavingMetric\n",
    "print('Loading Model...')\n",
    "input_dir ='saved_model_FD_UNet_5i-10j/best_models_from_model_dir_ssim_psnr'\n",
    "USE_MSE = True\n",
    "file_list = [file for file in os.listdir(input_dir) if os.path.isfile(os.path.join(input_dir, file))]\n",
    "filename = sorted(file_list, key = lambda x : int(x.partition('h_')[2].partition('-S')[0]))[-1]\n",
    "directory = os.path.join(os.getcwd(), input_dir)\n",
    "directory = os.path.join(directory, filename)\n",
    "fourier_loss = model_loss(B1=0.0, B2=1.0, name='fourier_loss')\n",
    "save_func = model_loss_experimental(B1=0,B2=0,B3=1, mse = USE_MSE, name='save_func')\n",
    "ssim_psnr = model_loss_experimental(B1=0,B2=0,B3=1, mse = USE_MSE, name='ssim_psnr')\n",
    "saved_model = tf.keras.models.load_model(directory, custom_objects={'model_loss':model_loss,'loss_func':model_loss(B1=0.99,B2=0.01), \n",
    "                                                                    'PSNR':PSNR, 'SSIM':SSIM, 'KLDivergence':KLDivergence, \n",
    "                                                                    'SavingMetric':SavingMetric, 'fourier_loss':fourier_loss, \n",
    "                                                                    'save_func':save_func, 'ssim_psnr':ssim_psnr})\n",
    "print('Done Loading Best Model (' + filename + ') from: ' + input_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Zero-Fill on Full Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Statements\n",
    "from utils.patchwork_alg import apply_model_patchwork\n",
    "# Quantitative Measurements\n",
    "from utils.model_utils import PSNR\n",
    "from utils.model_utils import SSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = 'display_image'\n",
    "file_list = os.listdir(dir_name)   \n",
    "filename = file_list[0]\n",
    "filepath = os.path.join(dir_name, filename)\n",
    "print('Directory: ' + filepath)\n",
    "downsampling_ratio = [5,10]\n",
    "orig_img = np.array(imageio.imread(filepath), dtype=np.float32)\n",
    "full_samp_img = exposure.rescale_intensity(orig_img[...,0], in_range='image', out_range=(0.0,1.0))\n",
    "'''\n",
    "# If using color images\n",
    "if len(img.shape)>2:\n",
    "    img = np.mean(img, axis = 2)\n",
    "'''\n",
    "# To Change Aspect Ratio of Image Before Running Model/Interpolation\n",
    "i_ratio = 2\n",
    "j_ratio = 1\n",
    "full_samp_img_shape = (full_samp_img.shape[0]*i_ratio, full_samp_img.shape[1]*j_ratio)\n",
    "full_samp_img = skimage.transform.resize(full_samp_img, output_shape=full_samp_img_shape, order=3, \n",
    "                                         mode='reflect', cval=0, clip=True, preserve_range=True, \n",
    "                                         anti_aliasing=True, anti_aliasing_sigma=None)\n",
    "latent_image = full_samp_img[::downsampling_ratio[0],::downsampling_ratio[1]]\n",
    "mask = np.zeros(full_samp_img.shape, dtype=np.int8)\n",
    "mask[::downsampling_ratio[0], ::downsampling_ratio[1]] = 1\n",
    "down_image = full_samp_img*mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(latent_image.shape)\n",
    "figsize = (15,15)\n",
    "fontsize = 23\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax.imshow(latent_image, cmap = 'gray')\n",
    "plt.title('Original Latent Image', fontsize=fontsize)\n",
    "plt.show()\n",
    "\n",
    "print(full_samp_img.shape)\n",
    "figsize = (30,30)\n",
    "fontsize = 23\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax.imshow(full_samp_img, cmap = 'gray')\n",
    "plt.title('Original Full-Sampled Image', fontsize=fontsize)\n",
    "plt.show()\n",
    "deep_image = apply_model_patchwork(saved_model, down_image = latent_image, downsampling_ratio = downsampling_ratio, \n",
    "                                   downsampling_axis = 'both', shape_for_model = (128,128), \n",
    "                                   buffer = 32, output_shape = full_samp_img.shape, interp=False)\n",
    "\n",
    "'''\n",
    "# To Perform Contrast Adjustment\n",
    "deep_image = skimage.img_as_float(deep_image)\n",
    "p1, p2 = np.percentile(deep_image, (0.01, 99.99))\n",
    "deep_image = exposure.rescale_intensity(deep_image, in_range=(p1, p2), out_range=(0.0,1.0))\n",
    "#'''\n",
    "\n",
    "figsize = (30,30)\n",
    "fontsize = 23\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax.imshow(deep_image, cmap = 'gray')\n",
    "plt.title('Cleaned DL Image', fontsize=fontsize)\n",
    "plt.show()\n",
    "\n",
    "# COMPARISON TO INTERPOLATION:\n",
    "interp_img = skimage.transform.resize(latent_image, output_shape=full_samp_img.shape, \n",
    "                                      order=3, mode='reflect', cval=0, clip=True, preserve_range=True, anti_aliasing=True, \n",
    "                                      anti_aliasing_sigma=None)\n",
    "print(interp_img.shape)\n",
    "figsize = (30,30)\n",
    "fontsize = 23\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax.imshow(interp_img, cmap = 'gray')\n",
    "plt.title('Bi-Cubic Interpolation Image', fontsize=fontsize)\n",
    "plt.show()\n",
    "\n",
    "# Saving Result Images:\n",
    "model_name = saved_model.name.replace(' ', '-')\n",
    "directory = f'{model_name}_figures_{downsampling_ratio[0]}i-{downsampling_ratio[1]}j_down_v4'\n",
    "print(f'Saving to: {directory}')\n",
    "if not os.path.exists(directory):\n",
    "    os.mkdir(directory)\n",
    "else:\n",
    "    shutil.rmtree(directory)\n",
    "    os.mkdir(directory)\n",
    "filename = 'latent_image.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "latent_image = exposure.rescale_intensity(latent_image, in_range='image', out_range=(0,255)).astype(np.uint8)\n",
    "imageio.imwrite(filepath, latent_image, format='.png', compress_level=0)\n",
    "filename = 'deep_image.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "deep_image = exposure.rescale_intensity(deep_image, in_range='image', out_range=(0,255)).astype(np.uint8)\n",
    "imageio.imwrite(filepath, deep_image, format='.png', compress_level=0)\n",
    "filename = 'interp_img.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "interp_img = exposure.rescale_intensity(interp_img, in_range='image', out_range=(0,255)).astype(np.uint8)\n",
    "imageio.imwrite(filepath, interp_img, format='.png', compress_level=0)\n",
    "filename = 'full_sample_img.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "full_samp_img = exposure.rescale_intensity(full_samp_img, in_range='image', out_range=(0,255)).astype(np.uint8)\n",
    "imageio.imwrite(filepath, full_samp_img, format='.png', compress_level=0)\n",
    "filename = 'down_image.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "down_image = exposure.rescale_intensity(down_image, in_range='image', out_range=(0,255)).astype(np.uint8)\n",
    "imageio.imwrite(filepath, down_image, format='.png', compress_level=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing and Saving Cropped Image Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Result Images:\n",
    "\n",
    "'''\n",
    "IMAGE 1:\n",
    "'''\n",
    "\n",
    "ratio_0 = 1025/1280\n",
    "ratio_1 = 720/1280\n",
    "ratio_2 = 740/2048\n",
    "ratio_3 = 430/2048\n",
    "\n",
    "filename = 'down_image_crop_1.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "shape = full_samp_img.shape\n",
    "down_image_crop = down_image[int(shape[0]-(ratio_0*shape[0])):int(shape[0]-(ratio_1*shape[0])),\n",
    "                             int(shape[1]-(ratio_2*shape[1])):int(shape[1]-(ratio_3*shape[1]))]\n",
    "imageio.imwrite(filepath, down_image_crop, format='.png', compress_level=0)\n",
    "filename = 'deep_image_crop_1.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "deep_image_crop = deep_image[int(shape[0]-(ratio_0*shape[0])):int(shape[0]-(ratio_1*shape[0])),\n",
    "                             int(shape[1]-(ratio_2*shape[1])):int(shape[1]-(ratio_3*shape[1]))]\n",
    "imageio.imwrite(filepath, deep_image_crop, format='.png', compress_level=0)\n",
    "filename = 'interp_img_crop_1.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "interp_img_crop = interp_img[int(shape[0]-(ratio_0*shape[0])):int(shape[0]-(ratio_1*shape[0])),\n",
    "                             int(shape[1]-(ratio_2*shape[1])):int(shape[1]-(ratio_3*shape[1]))]\n",
    "imageio.imwrite(filepath, interp_img_crop, format='.png', compress_level=0)\n",
    "filename = 'latent_image_crop_1.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "latent_shape = latent_image.shape\n",
    "latent_image_crop = latent_image[int(latent_shape[0]-(ratio_0*latent_shape[0])):int(latent_shape[0]-(ratio_1*latent_shape[0])),\n",
    "                             int(latent_shape[1]-(ratio_2*latent_shape[1])):int(latent_shape[1]-(ratio_3*latent_shape[1]))]\n",
    "imageio.imwrite(filepath, latent_image_crop, format='.png', compress_level=0)\n",
    "filename = 'latent_image_crop_resized_1.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "latent_image_crop_resized = skimage.transform.resize(latent_image_crop, output_shape=down_image_crop.shape, \n",
    "                                                     order=1, mode='reflect', cval=0, clip=True, preserve_range=True, \n",
    "                                                     anti_aliasing=True, anti_aliasing_sigma=None)\n",
    "latent_image_crop_resized = exposure.rescale_intensity(latent_image_crop_resized, in_range='image', out_range=(0,255)).astype(np.uint8)\n",
    "imageio.imwrite(filepath, latent_image_crop_resized, format='.png', compress_level=0)\n",
    "filename = 'full_img_crop_1.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "full_samp_img_crop = full_samp_img[int(shape[0]-(ratio_0*shape[0])):int(shape[0]-(ratio_1*shape[0])),\n",
    "                                   int(shape[1]-(ratio_2*shape[1])):int(shape[1]-(ratio_3*shape[1]))]\n",
    "imageio.imwrite(filepath, full_samp_img_crop, format='.png', compress_level=0)\n",
    "\n",
    "'''\n",
    "Displaying Images:\n",
    "'''\n",
    "figsize = (7,7)\n",
    "fontsize = 18\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax.imshow(down_image_crop, cmap = 'gray')\n",
    "plt.title('Downsampled Image Crop 1', fontsize=fontsize)\n",
    "plt.show()\n",
    "\n",
    "figsize = (7,7)\n",
    "fontsize = 18\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax.imshow(latent_image_crop, cmap = 'gray')\n",
    "plt.title('Latent Image Crop 1', fontsize=fontsize)\n",
    "plt.show()\n",
    "\n",
    "figsize = (7,7)\n",
    "fontsize = 18\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax.imshow(latent_image_crop_resized, cmap = 'gray')\n",
    "plt.title('Latent Image Crop Resized 1', fontsize=fontsize)\n",
    "plt.show()\n",
    "\n",
    "figsize = (7,7)\n",
    "fontsize = 18\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax.imshow(full_samp_img_crop, cmap = 'gray')\n",
    "plt.title('Original Fully-Sampled Image Crop 1', fontsize=fontsize)\n",
    "plt.show()\n",
    "\n",
    "figsize = (7,7)\n",
    "fontsize = 18\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax.imshow(deep_image_crop, cmap = 'gray')\n",
    "plt.title('DL Reconstruction Crop 1', fontsize=fontsize)\n",
    "plt.show()\n",
    "\n",
    "figsize = (7,7)\n",
    "fontsize = 18\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax.imshow(interp_img_crop, cmap = 'gray')\n",
    "plt.title('Bicubic Interpolation Crop 1', fontsize=fontsize)\n",
    "plt.show()\n",
    "\n",
    "'''\n",
    "Showing Vessel Profiles:\n",
    "'''\n",
    "\n",
    "DISP_CONV_FACTOR = 5 # converts pixels to micron\n",
    "\n",
    "angle = -20\n",
    "interp_crop = interp_img_crop[:interp_img_crop.shape[0]//3, \n",
    "                              :interp_img_crop.shape[1]//3]\n",
    "interp_crop = skimage.transform.rotate(interp_crop, angle)\n",
    "dl_crop = deep_image_crop[:deep_image_crop.shape[0]//3, \n",
    "                          :deep_image_crop.shape[1]//3]\n",
    "dl_crop = skimage.transform.rotate(dl_crop, angle)\n",
    "full_crop = full_samp_img_crop[:full_samp_img_crop.shape[0]//3, \n",
    "                               :full_samp_img_crop.shape[1]//3]\n",
    "full_crop = skimage.transform.rotate(full_crop, angle)\n",
    "\n",
    "figsize = (7,7)\n",
    "fontsize = 18\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0.14,0.14,0.8,0.8])\n",
    "y_shape = full_crop.shape[0]\n",
    "x_shape = full_crop.shape[1]\n",
    "ax.plot(np.arange(0, 40, 1)*DISP_CONV_FACTOR, full_crop[55, 22:62], \n",
    "        np.arange(0, 40, 1)*DISP_CONV_FACTOR, interp_crop[55, 22:62],\n",
    "        np.arange(0, 40, 1)*DISP_CONV_FACTOR, dl_crop[55, 22:62])\n",
    "ax.legend(['Ground Truth', 'Bicubic', 'Deep Learning'], prop={'size': 15})\n",
    "plt.xlabel('Distance ($\\mu$m)', fontsize=fontsize)\n",
    "plt.ylabel('Normalized PA Amplitude', fontsize=fontsize)\n",
    "plt.xticks(fontsize = 14)\n",
    "plt.yticks(fontsize = 14)\n",
    "plt.grid()\n",
    "filename = 'vessel_compare_1.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "plt.savefig(filepath, dpi=100)\n",
    "plt.show()\n",
    "\n",
    "angle = -40\n",
    "interp_crop = interp_img_crop[:interp_img_crop.shape[0]//3, \n",
    "                              2*interp_img_crop.shape[1]//3:]\n",
    "interp_crop = skimage.transform.rotate(interp_crop, angle)\n",
    "dl_crop = deep_image_crop[:deep_image_crop.shape[0]//3, \n",
    "                          2*deep_image_crop.shape[1]//3:]\n",
    "dl_crop = skimage.transform.rotate(dl_crop, angle)\n",
    "full_crop = full_samp_img_crop[:full_samp_img_crop.shape[0]//3, \n",
    "                               2*full_samp_img_crop.shape[1]//3:]\n",
    "full_crop = skimage.transform.rotate(full_crop, angle)\n",
    "figsize = (7,7)\n",
    "fontsize = 18\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0.14,0.14,0.8,0.8])\n",
    "y_shape = full_crop.shape[0]\n",
    "x_shape = full_crop.shape[1]\n",
    "ax.plot(np.arange(0, 30, 1)*DISP_CONV_FACTOR, full_crop[67, 22:52], \n",
    "        np.arange(0, 30, 1)*DISP_CONV_FACTOR, interp_crop[67, 22:52],\n",
    "        np.arange(0, 30, 1)*DISP_CONV_FACTOR, dl_crop[67, 22:52])\n",
    "ax.legend(['Ground Truth', 'Bicubic', 'Deep Learning'], prop={'size': 15})\n",
    "plt.xlabel('Distance ($\\mu$m)', fontsize=fontsize)\n",
    "plt.ylabel('Normalized PA Amplitude', fontsize=fontsize)\n",
    "plt.xticks(fontsize = 14)\n",
    "plt.yticks(fontsize = 14)\n",
    "plt.grid()\n",
    "filename = 'vessel_compare_2.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "plt.savefig(filepath, dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "IMAGE 2:\n",
    "'''\n",
    "ratio_0 = 1025/1280\n",
    "ratio_1 = 720/1280\n",
    "ratio_2 = 1405/2048\n",
    "ratio_3 = 1100/2048\n",
    "\n",
    "filename = 'down_image_crop_2.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "shape = full_samp_img.shape\n",
    "latent_shape = latent_image.shape\n",
    "down_image_crop = down_image[int(shape[0]-(ratio_0*shape[0])):int(shape[0]-(ratio_1*shape[0])),\n",
    "                             int(shape[1]-(ratio_2*shape[1])):int(shape[1]-(ratio_3*shape[1]))]\n",
    "imageio.imwrite(filepath, down_image_crop, format='.png', compress_level=0)\n",
    "filename = 'deep_image_crop_2.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "shape = full_samp_img.shape\n",
    "latent_shape = latent_image.shape\n",
    "deep_image_crop = deep_image[int(shape[0]-(ratio_0*shape[0])):int(shape[0]-(ratio_1*shape[0])),\n",
    "                             int(shape[1]-(ratio_2*shape[1])):int(shape[1]-(ratio_3*shape[1]))]\n",
    "imageio.imwrite(filepath, deep_image_crop, format='.png', compress_level=0)\n",
    "filename = 'interp_img_crop_2.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "interp_img_crop = interp_img[int(shape[0]-(ratio_0*shape[0])):int(shape[0]-(ratio_1*shape[0])),\n",
    "                             int(shape[1]-(ratio_2*shape[1])):int(shape[1]-(ratio_3*shape[1]))]\n",
    "imageio.imwrite(filepath, interp_img_crop, format='.png', compress_level=0)\n",
    "filename = 'latent_image_crop_2.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "latent_image_crop = latent_image[int(latent_shape[0]-(ratio_0*latent_shape[0])):int(latent_shape[0]-(ratio_1*latent_shape[0])),\n",
    "                             int(latent_shape[1]-(ratio_2*latent_shape[1])):int(latent_shape[1]-(ratio_3*latent_shape[1]))]\n",
    "imageio.imwrite(filepath, latent_image_crop, format='.png', compress_level=0)\n",
    "filename = 'latent_image_crop_resized_2.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "latent_image_crop_resized = skimage.transform.resize(latent_image_crop, output_shape=down_image_crop.shape, \n",
    "                                                     order=1, mode='reflect', cval=0, clip=True, preserve_range=True, \n",
    "                                                     anti_aliasing=True, anti_aliasing_sigma=None)\n",
    "latent_image_crop_resized = exposure.rescale_intensity(latent_image_crop_resized, in_range='image', out_range=(0,255)).astype(np.uint8)\n",
    "imageio.imwrite(filepath, latent_image_crop_resized, format='.png', compress_level=0)\n",
    "filename = 'full_img_crop_2.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "full_samp_img_crop = full_samp_img[int(shape[0]-(ratio_0*shape[0])):int(shape[0]-(ratio_1*shape[0])),\n",
    "                                   int(shape[1]-(ratio_2*shape[1])):int(shape[1]-(ratio_3*shape[1]))]\n",
    "imageio.imwrite(filepath, full_samp_img_crop, format='.png', compress_level=0)\n",
    "\n",
    "\n",
    "'''\n",
    "Displaying Images:\n",
    "'''\n",
    "\n",
    "figsize = (7,7)\n",
    "fontsize = 18\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax.imshow(down_image_crop, cmap = 'gray')\n",
    "plt.title('Downsampled Image Crop 2', fontsize=fontsize)\n",
    "plt.show()\n",
    "\n",
    "figsize = (7,7)\n",
    "fontsize = 18\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax.imshow(latent_image_crop, cmap = 'gray')\n",
    "plt.title('Latent Image Crop 2', fontsize=fontsize)\n",
    "plt.show()\n",
    "\n",
    "figsize = (7,7)\n",
    "fontsize = 18\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax.imshow(latent_image_crop_resized, cmap = 'gray')\n",
    "plt.title('Latent Image Crop Resized 2', fontsize=fontsize)\n",
    "plt.show()\n",
    "\n",
    "figsize = (7,7)\n",
    "fontsize = 18\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax.imshow(full_samp_img_crop, cmap = 'gray')\n",
    "plt.title('Original Fully-Sampled Image Crop 2', fontsize=fontsize)\n",
    "plt.show()\n",
    "\n",
    "figsize = (7,7)\n",
    "fontsize = 18\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax.imshow(deep_image_crop, cmap = 'gray')\n",
    "plt.title('DL Reconstruction Crop 2', fontsize=fontsize)\n",
    "plt.show()\n",
    "\n",
    "figsize = (7,7)\n",
    "fontsize = 18\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax.imshow(interp_img_crop, cmap = 'gray')\n",
    "plt.title('Bicubic Interpolation Crop 2', fontsize=fontsize)\n",
    "plt.show()\n",
    "\n",
    "'''\n",
    "Showing Vessel Profiles:\n",
    "'''\n",
    "\n",
    "DISP_CONV_FACTOR = 5 # converts pixels to micron\n",
    "\n",
    "figsize = (7,7)\n",
    "fontsize = 18\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "y_shape = full_samp_img_crop.shape[0]\n",
    "x_shape = full_samp_img_crop.shape[1]\n",
    "ax.plot(np.arange(0, x_shape, 1)*DISP_CONV_FACTOR, full_samp_img_crop[:, x_shape//2]/255, \n",
    "        np.arange(0, x_shape, 1)*DISP_CONV_FACTOR, interp_img_crop[:, x_shape//2]/255,\n",
    "        np.arange(0, x_shape, 1)*DISP_CONV_FACTOR, deep_image_crop[:, x_shape//2]/255)\n",
    "ax.legend(['Ground Truth', 'Bicubic', 'Deep Learning'], prop={'size': 15})\n",
    "#plt.title('Vessel Profile Comparison 3', fontsize=fontsize)\n",
    "plt.xlabel('Distance ($\\mu$m)', fontsize=fontsize)\n",
    "plt.ylabel('Normalized PA Amplitude', fontsize=fontsize)\n",
    "plt.xticks(fontsize = 14)\n",
    "plt.yticks(fontsize = 14)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "figsize = (7,7)\n",
    "fontsize = 18\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "y_shape = full_samp_img_crop.shape[0]\n",
    "x_shape = full_samp_img_crop.shape[1]\n",
    "ax.plot(np.arange(0, x_shape, 1)*DISP_CONV_FACTOR, full_samp_img_crop[y_shape//3, :]/255, \n",
    "        np.arange(0, x_shape, 1)*DISP_CONV_FACTOR, interp_img_crop[y_shape//3, :]/255,\n",
    "        np.arange(0, x_shape, 1)*DISP_CONV_FACTOR, deep_image_crop[y_shape//3, :]/255)\n",
    "ax.legend(['Ground Truth', 'Bicubic', 'Deep Learning'], prop={'size': 15})\n",
    "#plt.title('Vessel Profile Comparison 4', fontsize=fontsize)\n",
    "plt.xlabel('Distance ($\\mu$m)', fontsize=fontsize)\n",
    "plt.ylabel('Normalized PA Amplitude', fontsize=fontsize)\n",
    "plt.xticks(fontsize = 14)\n",
    "plt.yticks(fontsize = 14)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "IMAGE 3:\n",
    "'''\n",
    "ratio_0 = 525/1280\n",
    "ratio_1 = 220/1280\n",
    "ratio_2 = 1405/2048\n",
    "ratio_3 = 1100/2048\n",
    "\n",
    "filename = 'down_image_crop_3.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "shape = full_samp_img.shape\n",
    "latent_shape = latent_image.shape\n",
    "down_image_crop = down_image[int(shape[0]-(ratio_0*shape[0])):int(shape[0]-(ratio_1*shape[0])),\n",
    "                             int(shape[1]-(ratio_2*shape[1])):int(shape[1]-(ratio_3*shape[1]))]\n",
    "imageio.imwrite(filepath, down_image_crop, format='.png', compress_level=0)\n",
    "filename = 'deep_image_crop_3.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "shape = full_samp_img.shape\n",
    "latent_shape = latent_image.shape\n",
    "deep_image_crop = deep_image[int(shape[0]-(ratio_0*shape[0])):int(shape[0]-(ratio_1*shape[0])),\n",
    "                             int(shape[1]-(ratio_2*shape[1])):int(shape[1]-(ratio_3*shape[1]))]\n",
    "imageio.imwrite(filepath, deep_image_crop, format='.png', compress_level=0)\n",
    "filename = 'interp_img_crop_3.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "interp_img_crop = interp_img[int(shape[0]-(ratio_0*shape[0])):int(shape[0]-(ratio_1*shape[0])),\n",
    "                             int(shape[1]-(ratio_2*shape[1])):int(shape[1]-(ratio_3*shape[1]))]\n",
    "imageio.imwrite(filepath, interp_img_crop, format='.png', compress_level=0)\n",
    "filename = 'latent_image_crop_3.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "latent_image_crop = latent_image[int(latent_shape[0]-(ratio_0*latent_shape[0])):int(latent_shape[0]-(ratio_1*latent_shape[0])),\n",
    "                             int(latent_shape[1]-(ratio_2*latent_shape[1])):int(latent_shape[1]-(ratio_3*latent_shape[1]))]\n",
    "imageio.imwrite(filepath, latent_image_crop, format='.png', compress_level=0)\n",
    "filename = 'latent_image_crop_resized_3.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "latent_image_crop_resized = skimage.transform.resize(latent_image_crop, output_shape=down_image_crop.shape, \n",
    "                                                     order=1, mode='reflect', cval=0, clip=True, preserve_range=True, \n",
    "                                                     anti_aliasing=True, anti_aliasing_sigma=None)\n",
    "latent_image_crop_resized = exposure.rescale_intensity(latent_image_crop_resized, in_range='image', out_range=(0,255)).astype(np.uint8)\n",
    "imageio.imwrite(filepath, latent_image_crop_resized, format='.png', compress_level=0)\n",
    "filename = 'full_img_crop_3.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "full_samp_img_crop = full_samp_img[int(shape[0]-(ratio_0*shape[0])):int(shape[0]-(ratio_1*shape[0])),\n",
    "                                   int(shape[1]-(ratio_2*shape[1])):int(shape[1]-(ratio_3*shape[1]))]\n",
    "imageio.imwrite(filepath, full_samp_img_crop, format='.png', compress_level=0)\n",
    "\n",
    "figsize = (7,7)\n",
    "fontsize = 18\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax.imshow(down_image_crop, cmap = 'gray')\n",
    "plt.title('Downsampled Image Crop 3', fontsize=fontsize)\n",
    "plt.show()\n",
    "\n",
    "figsize = (7,7)\n",
    "fontsize = 18\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax.imshow(latent_image_crop, cmap = 'gray')\n",
    "plt.title('Latent Image Crop 3', fontsize=fontsize)\n",
    "plt.show()\n",
    "\n",
    "figsize = (7,7)\n",
    "fontsize = 18\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax.imshow(latent_image_crop_resized, cmap = 'gray')\n",
    "plt.title('Latent Image Crop Resized 3', fontsize=fontsize)\n",
    "plt.show()\n",
    "\n",
    "figsize = (7,7)\n",
    "fontsize = 18\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax.imshow(full_samp_img_crop, cmap = 'gray')\n",
    "plt.title('Original Fully-Sampled Image Crop 3', fontsize=fontsize)\n",
    "plt.show()\n",
    "\n",
    "figsize = (7,7)\n",
    "fontsize = 18\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax.imshow(deep_image_crop, cmap = 'gray')\n",
    "plt.title('DL Reconstruction Crop 3', fontsize=fontsize)\n",
    "plt.show()\n",
    "\n",
    "figsize = (7,7)\n",
    "fontsize = 18\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax.imshow(interp_img_crop, cmap = 'gray')\n",
    "plt.title('Bicubic Interpolation Crop 3', fontsize=fontsize)\n",
    "plt.show()\n",
    "\n",
    "# Showing Vessel Profiles\n",
    "\n",
    "DISP_CONV_FACTOR = 5 # converts pixels to micron\n",
    "\n",
    "figsize = (7,7)\n",
    "fontsize = 18\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "y_shape = full_samp_img_crop.shape[0]\n",
    "x_shape = full_samp_img_crop.shape[1]\n",
    "ax.plot(np.arange(0, x_shape, 1)*DISP_CONV_FACTOR, full_samp_img_crop[:, x_shape//2]/255, \n",
    "        np.arange(0, x_shape, 1)*DISP_CONV_FACTOR, interp_img_crop[:, x_shape//2]/255,\n",
    "        np.arange(0, x_shape, 1)*DISP_CONV_FACTOR, deep_image_crop[:, x_shape//2]/255)\n",
    "ax.legend(['Ground Truth', 'Bicubic', 'Deep Learning'], prop={'size': 15})\n",
    "#plt.title('Vessel Profile Comparison 5', fontsize=fontsize)\n",
    "plt.xlabel('Distance ($\\mu$m)', fontsize=fontsize)\n",
    "plt.ylabel('Normalized PA Amplitude', fontsize=fontsize)\n",
    "plt.xticks(fontsize = 14)\n",
    "plt.yticks(fontsize = 14)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "figsize = (7,7)\n",
    "fontsize = 18\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "y_shape = full_samp_img_crop.shape[0]\n",
    "x_shape = full_samp_img_crop.shape[1]\n",
    "ax.plot(np.arange(0, x_shape, 1)*DISP_CONV_FACTOR, full_samp_img_crop[y_shape//3, :]/255, \n",
    "        np.arange(0, x_shape, 1)*DISP_CONV_FACTOR, interp_img_crop[y_shape//3, :]/255,\n",
    "        np.arange(0, x_shape, 1)*DISP_CONV_FACTOR, deep_image_crop[y_shape//3, :]/255)\n",
    "ax.legend(['Ground Truth', 'Bicubic', 'Deep Learning'], prop={'size': 15})\n",
    "#plt.title('Vessel Profile Comparison 6', fontsize=fontsize)\n",
    "plt.xlabel('Distance ($\\mu$m)', fontsize=fontsize)\n",
    "plt.ylabel('Normalized PA Amplitude', fontsize=fontsize)\n",
    "plt.xticks(fontsize = 14)\n",
    "plt.yticks(fontsize = 14)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "figsize = (7,7)\n",
    "fontsize = 18\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "y_shape = full_samp_img_crop.shape[0]\n",
    "x_shape = full_samp_img_crop.shape[1]\n",
    "ax.plot(np.arange(0, x_shape, 1)*DISP_CONV_FACTOR, full_samp_img_crop[y_shape//6, :]/255, \n",
    "        np.arange(0, x_shape, 1)*DISP_CONV_FACTOR, interp_img_crop[y_shape//6, :]/255,\n",
    "        np.arange(0, x_shape, 1)*DISP_CONV_FACTOR, deep_image_crop[y_shape//6, :]/255)\n",
    "ax.legend(['Ground Truth', 'Bicubic', 'Deep Learning'], prop={'size': 15})\n",
    "#plt.title('Vessel Profile Comparison 6', fontsize=fontsize)\n",
    "plt.xlabel('Distance ($\\mu$m)', fontsize=fontsize)\n",
    "plt.ylabel('Normalized PA Amplitude', fontsize=fontsize)\n",
    "plt.xticks(fontsize = 14)\n",
    "plt.yticks(fontsize = 14)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "IMAGE 4:\n",
    "'''\n",
    "ratio_0 = 625/1280\n",
    "ratio_1 = 320/1280\n",
    "ratio_2 = 935/2048\n",
    "ratio_3 = 630/2048\n",
    "\n",
    "filename = 'down_image_crop_4.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "shape = full_samp_img.shape\n",
    "latent_shape = latent_image.shape\n",
    "down_image_crop = down_image[int(shape[0]-(ratio_0*shape[0])):int(shape[0]-(ratio_1*shape[0])),\n",
    "                             int(shape[1]-(ratio_2*shape[1])):int(shape[1]-(ratio_3*shape[1]))]\n",
    "imageio.imwrite(filepath, down_image_crop, format='.png', compress_level=0)\n",
    "filename = 'deep_image_crop_4.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "shape = full_samp_img.shape\n",
    "latent_shape = latent_image.shape\n",
    "deep_image_crop = deep_image[int(shape[0]-(ratio_0*shape[0])):int(shape[0]-(ratio_1*shape[0])),\n",
    "                             int(shape[1]-(ratio_2*shape[1])):int(shape[1]-(ratio_3*shape[1]))]\n",
    "imageio.imwrite(filepath, deep_image_crop, format='.png', compress_level=0)\n",
    "filename = 'interp_img_crop_4.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "interp_img_crop = interp_img[int(shape[0]-(ratio_0*shape[0])):int(shape[0]-(ratio_1*shape[0])),\n",
    "                             int(shape[1]-(ratio_2*shape[1])):int(shape[1]-(ratio_3*shape[1]))]\n",
    "imageio.imwrite(filepath, interp_img_crop, format='.png', compress_level=0)\n",
    "filename = 'latent_image_crop_4.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "latent_image_crop = latent_image[int(latent_shape[0]-(ratio_0*latent_shape[0])):int(latent_shape[0]-(ratio_1*latent_shape[0])),\n",
    "                             int(latent_shape[1]-(ratio_2*latent_shape[1])):int(latent_shape[1]-(ratio_3*latent_shape[1]))]\n",
    "imageio.imwrite(filepath, latent_image_crop, format='.png', compress_level=0)\n",
    "filename = 'latent_image_crop_resized_4.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "latent_image_crop_resized = skimage.transform.resize(latent_image_crop, output_shape=down_image_crop.shape, \n",
    "                                                     order=1, mode='reflect', cval=0, clip=True, preserve_range=True, \n",
    "                                                     anti_aliasing=True, anti_aliasing_sigma=None)\n",
    "latent_image_crop_resized = exposure.rescale_intensity(latent_image_crop_resized, in_range='image', out_range=(0,255)).astype(np.uint8)\n",
    "imageio.imwrite(filepath, latent_image_crop_resized, format='.png', compress_level=0)\n",
    "filename = 'full_img_crop_4.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "full_samp_img_crop = full_samp_img[int(shape[0]-(ratio_0*shape[0])):int(shape[0]-(ratio_1*shape[0])),\n",
    "                                   int(shape[1]-(ratio_2*shape[1])):int(shape[1]-(ratio_3*shape[1]))]\n",
    "imageio.imwrite(filepath, full_samp_img_crop, format='.png', compress_level=0)\n",
    "\n",
    "figsize = (7,7)\n",
    "fontsize = 18\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax.imshow(down_image_crop, cmap = 'gray')\n",
    "plt.title('Downsampled Image Crop 4', fontsize=fontsize)\n",
    "plt.show()\n",
    "\n",
    "figsize = (7,7)\n",
    "fontsize = 18\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax.imshow(latent_image_crop, cmap = 'gray')\n",
    "plt.title('Latent Image Crop 4', fontsize=fontsize)\n",
    "plt.show()\n",
    "\n",
    "figsize = (7,7)\n",
    "fontsize = 18\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax.imshow(latent_image_crop_resized, cmap = 'gray')\n",
    "plt.title('Latent Image Crop Resized 4', fontsize=fontsize)\n",
    "plt.show()\n",
    "\n",
    "figsize = (7,7)\n",
    "fontsize = 18\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax.imshow(full_samp_img_crop, cmap = 'gray')\n",
    "plt.title('Original Fully-Sampled Image Crop 4', fontsize=fontsize)\n",
    "plt.show()\n",
    "\n",
    "figsize = (7,7)\n",
    "fontsize = 18\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax.imshow(deep_image_crop, cmap = 'gray')\n",
    "plt.title('DL Reconstruction Crop 4', fontsize=fontsize)\n",
    "plt.show()\n",
    "\n",
    "figsize = (7,7)\n",
    "fontsize = 18\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax.imshow(interp_img_crop, cmap = 'gray')\n",
    "plt.title('Bicubic Interpolation Crop 4', fontsize=fontsize)\n",
    "plt.show()\n",
    "\n",
    "# Showing Vessel Profiles\n",
    "\n",
    "DISP_CONV_FACTOR = 5 # converts pixels to micron\n",
    "\n",
    "figsize = (7,7)\n",
    "fontsize = 18\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "y_shape = full_samp_img_crop.shape[0]\n",
    "x_shape = full_samp_img_crop.shape[1]\n",
    "ax.plot(np.arange(0, x_shape, 1)*DISP_CONV_FACTOR, full_samp_img_crop[:, x_shape//2]/255, \n",
    "        np.arange(0, x_shape, 1)*DISP_CONV_FACTOR, interp_img_crop[:, x_shape//2]/255,\n",
    "        np.arange(0, x_shape, 1)*DISP_CONV_FACTOR, deep_image_crop[:, x_shape//2]/255)\n",
    "ax.legend(['Ground Truth', 'Bicubic', 'Deep Learning'], prop={'size': 15})\n",
    "#plt.title('Vessel Profile Comparison 7, fontsize=fontsize')\n",
    "plt.xlabel('Distance ($\\mu$m)', fontsize=18)\n",
    "plt.ylabel('Normalized PA Amplitude', fontsize=18)\n",
    "plt.xticks(fontsize = 14)\n",
    "plt.yticks(fontsize = 14)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "figsize = (7,7)\n",
    "fontsize = 18\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "y_shape = full_samp_img_crop.shape[0]\n",
    "x_shape = full_samp_img_crop.shape[1]\n",
    "ax.plot(np.arange(0, x_shape, 1)*DISP_CONV_FACTOR, full_samp_img_crop[y_shape//3, :]/255, \n",
    "        np.arange(0, x_shape, 1)*DISP_CONV_FACTOR, interp_img_crop[y_shape//3, :]/255,\n",
    "        np.arange(0, x_shape, 1)*DISP_CONV_FACTOR, deep_image_crop[y_shape//3, :]/255)\n",
    "ax.legend(['Ground Truth', 'Bicubic', 'Deep Learning'], prop={'size': 15})\n",
    "#plt.title('Vessel Profile Comparison 8, fontsize=fontsize')\n",
    "plt.xlabel('Distance ($\\mu$m)', fontsize=fontsize)\n",
    "plt.ylabel('Normalized PA Amplitude', fontsize=fontsize)\n",
    "plt.xticks(fontsize = 14)\n",
    "plt.yticks(fontsize = 14)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "figsize = (7,7)\n",
    "fontsize = 18\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "y_shape = full_samp_img_crop.shape[0]\n",
    "x_shape = full_samp_img_crop.shape[1]\n",
    "ax.plot(np.arange(0, 23, 1)*DISP_CONV_FACTOR, full_samp_img_crop[y_shape//3, 62:85]/255, \n",
    "        np.arange(0, 23, 1)*DISP_CONV_FACTOR, interp_img_crop[y_shape//3, 62:85]/255,\n",
    "        np.arange(0, 23, 1)*DISP_CONV_FACTOR, deep_image_crop[y_shape//3, 62:85]/255)\n",
    "ax.legend(['Ground Truth', 'Bicubic', 'Deep Learning'], prop={'size': 15})\n",
    "#plt.title('Vessel Profile Comparison 8, fontsize=fontsize')\n",
    "plt.xlabel('Distance ($\\mu$m)', fontsize=fontsize)\n",
    "plt.ylabel('Normalized PA Amplitude', fontsize=fontsize)\n",
    "plt.xticks(fontsize = 14)\n",
    "plt.yticks(fontsize = 14)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Statistical Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantitative Measurements\n",
    "from utils.model_utils import PSNR\n",
    "from utils.model_utils import SSIM\n",
    "deep_image = deep_image[..., None]\n",
    "deep_image = tf.image.convert_image_dtype(deep_image[None, ...], tf.float32)\n",
    "interp_img = interp_img[..., None]\n",
    "interp_img = tf.image.convert_image_dtype(interp_img[None, ...], tf.float32)\n",
    "full_samp_img = full_samp_img[..., None]\n",
    "full_samp_img = tf.image.convert_image_dtype(full_samp_img[None, ...], tf.float32)\n",
    "print('---------------------------------------')\n",
    "print('##########LIST OF STATISTICS:##########')\n",
    "print('---------------------------------------')\n",
    "print('COMPARISON OF PSNR:')\n",
    "print('Deep Learning: ' + str(PSNR(full_samp_img, deep_image).numpy()[0]))\n",
    "print('Bicubic Interp: ' + str(PSNR(full_samp_img, interp_img).numpy()[0]))\n",
    "print('COMPARISON OF SSIM:')\n",
    "print('Deep Learning: ' + str(SSIM(full_samp_img, deep_image).numpy()[0]))\n",
    "print('Bicubic Interp: ' + str(SSIM(full_samp_img, interp_img).numpy()[0]))\n",
    "print('COMPARISON OF MEAN ABSOLUTE ERROR:')\n",
    "MAE = tf.keras.losses.MeanAbsoluteError()\n",
    "print('Deep Learning: ' + str(MAE(full_samp_img, deep_image).numpy()))\n",
    "print('Bicubic Interp: ' + str(MAE(full_samp_img, interp_img).numpy()))\n",
    "print('COMPARISON OF MEAN SQUARED ERROR:')\n",
    "MSE = tf.keras.losses.MeanSquaredError()\n",
    "print('Deep Learning: ' + str(MSE(full_samp_img, deep_image).numpy()))\n",
    "print('Bicubic Interp: ' + str(MSE(full_samp_img, interp_img).numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Creating Training Data where Downsampling is Performed and then using Various Pre-Reconstruction Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Fully Sampled Data from Previous \"Data\" Folder to Create Folders With Different Downsampling Ratios While Maintaining the Same Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Statement\n",
    "from utils.data_folder_utils import extract_full_samp_and_retain_file_struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'data'\n",
    "output_dir = 'data_1_5'\n",
    "extract_full_samp_and_retain_file_struct(input_dir, output_dir, delete_previous = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZERO FILL METHOD - Going Through Directory to Downsample and Fill in Missing Pixels with Zero Fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Statements:\n",
    "import utils.standardize_dir_utils\n",
    "from utils.standardize_dir_utils import pad_img_and_add_down_channel\n",
    "from utils.standardize_dir_utils import standardize_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STANDARD_IMAGE_SHAPE = (128,128,1)\n",
    "DOWN_RATIO = [1,5]\n",
    "main_dir = 'data_1_5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = main_dir + '/train/input'\n",
    "total_train_image_pool = standardize_dir(input_dir = INPUT_DIR, downsample_axis = 'both', downsample_ratio = DOWN_RATIO, \n",
    "                                         standard_shape = STANDARD_IMAGE_SHAPE, file_format = '.tif', add_down_ratio = True)\n",
    "print('total_train_image_pool = ' + str(total_train_image_pool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = main_dir + '/val/input'\n",
    "total_val_image_pool = standardize_dir(input_dir = INPUT_DIR, downsample_axis = 'both', downsample_ratio = DOWN_RATIO, \n",
    "                                       standard_shape = STANDARD_IMAGE_SHAPE, file_format = '.tif', add_down_ratio = True)\n",
    "print('total_val_image_pool = ' + str(total_val_image_pool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = main_dir + '/test/input'\n",
    "total_test_image_pool = standardize_dir(input_dir = INPUT_DIR, downsample_axis = 'both', downsample_ratio = DOWN_RATIO, \n",
    "                                        standard_shape = STANDARD_IMAGE_SHAPE, file_format = '.tif', add_down_ratio = True)\n",
    "print('total_test_image_pool = ' + str(total_test_image_pool))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZERO FILL (WITH BLUR) METHOD - Going Through Directory to Downsample, Filling in Missing Pixels with Zero Fill, and then applying Gaussian blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Statements:\n",
    "import utils.standardize_dir_utils\n",
    "from utils.standardize_dir_utils import pad_img_and_add_down_channel\n",
    "from utils.standardize_dir_utils import standardize_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STANDARD_IMAGE_SHAPE = (128,128,1)\n",
    "DOWN_RATIO = [1,5]\n",
    "BLUR = 2.0\n",
    "main_dir = 'data_1_5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = main_dir + '/train/input'\n",
    "total_train_image_pool = standardize_dir(input_dir = INPUT_DIR, downsample_axis = 'both', downsample_ratio = DOWN_RATIO, \n",
    "                                         standard_shape = STANDARD_IMAGE_SHAPE, file_format = '.tif', add_down_ratio = True, gauss_blur_std=BLUR)\n",
    "print('total_train_image_pool = ' + str(total_train_image_pool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = main_dir + '/val/input'\n",
    "total_val_image_pool = standardize_dir(input_dir = INPUT_DIR, downsample_axis = 'both', downsample_ratio = DOWN_RATIO, \n",
    "                                       standard_shape = STANDARD_IMAGE_SHAPE, file_format = '.tif', add_down_ratio = True, gauss_blur_std=BLUR)\n",
    "print('total_val_image_pool = ' + str(total_val_image_pool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = main_dir + '/test/input'\n",
    "total_test_image_pool = standardize_dir(input_dir = INPUT_DIR, downsample_axis = 'both', downsample_ratio = DOWN_RATIO, \n",
    "                                        standard_shape = STANDARD_IMAGE_SHAPE, file_format = '.tif', add_down_ratio = True, gauss_blur_std=BLUR)\n",
    "print('total_test_image_pool = ' + str(total_test_image_pool))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BICUBIC INTERPOLATION METHOD - Going Through Directory to Downsample and then Partially Reconstructing Using Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Statements:\n",
    "import utils.standardize_dir_utils\n",
    "from utils.standardize_dir_utils import pad_img_and_add_interp_down_channel\n",
    "from utils.standardize_dir_utils import standardize_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STANDARD_IMAGE_SHAPE = (128,128,1)\n",
    "DOWN_RATIO = [1,5]\n",
    "INTERPOLATION = True\n",
    "main_dir = 'data_1_5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = main_dir + '/train/input'\n",
    "total_train_image_pool = standardize_dir(input_dir = INPUT_DIR, downsample_axis = 'both', downsample_ratio = DOWN_RATIO, \n",
    "                                         standard_shape = STANDARD_IMAGE_SHAPE, file_format = '.tif', add_down_ratio = True,\n",
    "                                         interp = INTERPOLATION)\n",
    "print('total_train_image_pool = ' + str(total_train_image_pool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = main_dir + '/val/input'\n",
    "total_val_image_pool = standardize_dir(input_dir = INPUT_DIR, downsample_axis = 'both', downsample_ratio = DOWN_RATIO, \n",
    "                                       standard_shape = STANDARD_IMAGE_SHAPE, file_format = '.tif', add_down_ratio = True,\n",
    "                                       interp = INTERPOLATION)\n",
    "print('total_val_image_pool = ' + str(total_val_image_pool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = main_dir + '/test/input'\n",
    "total_test_image_pool = standardize_dir(input_dir = INPUT_DIR, downsample_axis = 'both', downsample_ratio = DOWN_RATIO, \n",
    "                                        standard_shape = STANDARD_IMAGE_SHAPE, file_format = '.tif', add_down_ratio = True,\n",
    "                                        interp = INTERPOLATION)\n",
    "print('total_test_image_pool = ' + str(total_test_image_pool))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Images in New Dir (without padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.remove_pad_save_to_new_dir\n",
    "OUTPUT_DIR = 'no_pad_data-5_10'\n",
    "file_format = '.png'\n",
    "NUM_IMAGES = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = 'data/test/input'\n",
    "remove_pad_save_to_new_dir(INPUT_DIR, OUTPUT_DIR, file_format, NUM_IMAGES)\n",
    "print('Done Saving!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Creating Test Images for *Interpolation Pre-Reconstruction* Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model_utils import model_loss\n",
    "from utils.model_utils import model_loss_experimental\n",
    "from utils.model_utils import PSNR\n",
    "from utils.model_utils import SSIM\n",
    "from utils.model_utils import KLDivergence\n",
    "from utils.model_utils import SavingMetric\n",
    "print('Loading Model...')\n",
    "input_dir = 'saved_model_FD_UNet_1i-5j\\\\best_models'\n",
    "USE_MSE = True\n",
    "file_list = [file for file in os.listdir(input_dir) if os.path.isfile(os.path.join(input_dir, file))]\n",
    "filename = sorted(file_list, key = lambda x : int(x.partition('h_')[2].partition('-S')[0]))[-1]\n",
    "directory = os.path.join(os.getcwd(), input_dir)\n",
    "directory = os.path.join(directory, filename)\n",
    "fourier_loss = model_loss(B1=0.0, B2=1.0, name='fourier_loss')\n",
    "save_func = model_loss_experimental(B1=0,B2=0,B3=1, mse = USE_MSE, name='save_func')\n",
    "ssim_psnr = model_loss_experimental(B1=0,B2=0,B3=1, mse = USE_MSE, name='ssim_psnr')\n",
    "saved_model = tf.keras.models.load_model(directory, custom_objects={'model_loss':model_loss,'loss_func':model_loss(B1=0.99,B2=0.01), \n",
    "                                                                    'PSNR':PSNR, 'SSIM':SSIM, 'KLDivergence':KLDivergence, \n",
    "                                                                    'SavingMetric':SavingMetric, 'fourier_loss':fourier_loss, \n",
    "                                                                    'save_func':save_func, 'ssim_psnr':ssim_psnr})\n",
    "print('Done Loading Best Model (' + filename + ') from: ' + input_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Statements\n",
    "from utils.patchwork_alg import apply_model_patchwork\n",
    "# Quantitative Measurements\n",
    "from utils.model_utils import PSNR\n",
    "from utils.model_utils import SSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = 'display_image'\n",
    "file_list = os.listdir(dir_name)   \n",
    "filename = file_list[0]\n",
    "filepath = os.path.join(dir_name, filename)\n",
    "print('Directory: ' + filepath)\n",
    "downsampling_ratio = [5,10]\n",
    "orig_img = np.array(imageio.imread(filepath), dtype=np.float32)\n",
    "full_samp_img = exposure.rescale_intensity(orig_img[...,0], in_range='image', out_range=(0.0,1.0))\n",
    "'''\n",
    "# If using color images\n",
    "if len(img.shape)>2:\n",
    "    img = np.mean(img, axis = 2)\n",
    "'''\n",
    "# To Change Aspect Ratio of Image Before Running Model/Interpolation\n",
    "i_ratio = 2\n",
    "j_ratio = 1\n",
    "full_samp_img_shape = (full_samp_img.shape[0]*i_ratio, full_samp_img.shape[1]*j_ratio)\n",
    "full_samp_img = skimage.transform.resize(full_samp_img, output_shape=full_samp_img_shape, order=3, \n",
    "                                         mode='reflect', cval=0, clip=True, preserve_range=True, \n",
    "                                         anti_aliasing=True, anti_aliasing_sigma=None)\n",
    "latent_image = full_samp_img[::downsampling_ratio[0],::downsampling_ratio[1]]\n",
    "mask = np.zeros(full_samp_img.shape, dtype=np.int8)\n",
    "mask[::downsampling_ratio[0], ::downsampling_ratio[1]] = 1\n",
    "down_image = full_samp_img*mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(latent_image.shape)\n",
    "figsize = (15,15)\n",
    "fontsize = 23\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax.imshow(latent_image, cmap = 'gray')\n",
    "plt.title('Original Latent Image', fontsize=fontsize)\n",
    "plt.show()\n",
    "\n",
    "print(full_samp_img.shape)\n",
    "figsize = (30,30)\n",
    "fontsize = 23\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax.imshow(full_samp_img, cmap = 'gray')\n",
    "plt.title('Original Full-Sampled Image', fontsize=fontsize)\n",
    "plt.show()\n",
    "deep_image = apply_model_patchwork(saved_model, down_image = latent_image, downsampling_ratio = downsampling_ratio, \n",
    "                                   downsampling_axis = 'both', shape_for_model = (128,128), \n",
    "                                   buffer = 32, output_shape = full_samp_img.shape, interp=True)\n",
    "\n",
    "'''\n",
    "# To Perform Contrast Adjustment\n",
    "deep_image = skimage.img_as_float(deep_image)\n",
    "p1, p2 = np.percentile(deep_image, (0.01, 99.99))\n",
    "deep_image = exposure.rescale_intensity(deep_image, in_range=(p1, p2), out_range=(0.0,1.0))\n",
    "#'''\n",
    "\n",
    "figsize = (30,30)\n",
    "fontsize = 23\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax.imshow(deep_image, cmap = 'gray')\n",
    "plt.title('Cleaned DL Image', fontsize=fontsize)\n",
    "plt.show()\n",
    "\n",
    "# COMPARISON TO INTERPOLATION:\n",
    "interp_img = skimage.transform.resize(latent_image, output_shape=full_samp_img.shape, \n",
    "                                      order=3, mode='reflect', cval=0, clip=True, preserve_range=True, anti_aliasing=True, \n",
    "                                      anti_aliasing_sigma=None)\n",
    "print(interp_img.shape)\n",
    "figsize = (30,30)\n",
    "fontsize = 23\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax.imshow(interp_img, cmap = 'gray')\n",
    "plt.title('Bi-Cubic Interpolation Image', fontsize=fontsize)\n",
    "plt.show()\n",
    "\n",
    "# Saving Result Images:\n",
    "model_name = saved_model.name.replace(' ', '-')\n",
    "directory = f'{model_name}_figures_{downsampling_ratio[0]}i-{downsampling_ratio[1]}j_down_interp'\n",
    "print(f'Saving to: {directory}')\n",
    "if not os.path.exists(directory):\n",
    "    os.mkdir(directory)\n",
    "else:\n",
    "    shutil.rmtree(directory)\n",
    "    os.mkdir(directory)\n",
    "filename = 'latent_image.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "latent_image = exposure.rescale_intensity(latent_image, in_range='image', out_range=(0,255)).astype(np.uint8)\n",
    "imageio.imwrite(filepath, latent_image, format='.png', compress_level=0)\n",
    "filename = 'deep_image.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "deep_image = exposure.rescale_intensity(deep_image, in_range='image', out_range=(0,255)).astype(np.uint8)\n",
    "imageio.imwrite(filepath, deep_image, format='.png', compress_level=0)\n",
    "filename = 'interp_img.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "interp_img = exposure.rescale_intensity(interp_img, in_range='image', out_range=(0,255)).astype(np.uint8)\n",
    "imageio.imwrite(filepath, interp_img, format='.png', compress_level=0)\n",
    "filename = 'full_sample_img.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "full_samp_img = exposure.rescale_intensity(full_samp_img, in_range='image', out_range=(0,255)).astype(np.uint8)\n",
    "imageio.imwrite(filepath, full_samp_img, format='.png', compress_level=0)\n",
    "filename = 'down_image.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "down_image = exposure.rescale_intensity(down_image, in_range='image', out_range=(0,255)).astype(np.uint8)\n",
    "imageio.imwrite(filepath, down_image, format='.png', compress_level=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing and Saving Cropped Image Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Result Images:\n",
    "\n",
    "'''\n",
    "IMAGE 1:\n",
    "'''\n",
    "ratio_0 = 1025/1280\n",
    "ratio_1 = 720/1280\n",
    "ratio_2 = 740/2048\n",
    "ratio_3 = 430/2048\n",
    "\n",
    "filename = 'deep_image_crop_1.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "shape = full_samp_img.shape\n",
    "latent_shape = latent_image.shape\n",
    "deep_image_crop = deep_image[int(shape[0]-(ratio_0*shape[0])):int(shape[0]-(ratio_1*shape[0])),\n",
    "                             int(shape[1]-(ratio_2*shape[1])):int(shape[1]-(ratio_3*shape[1]))]\n",
    "imageio.imwrite(filepath, deep_image_crop, format='.png', compress_level=0)\n",
    "filename = 'interp_img_crop_1.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "interp_img_crop = interp_img[int(shape[0]-(ratio_0*shape[0])):int(shape[0]-(ratio_1*shape[0])),\n",
    "                             int(shape[1]-(ratio_2*shape[1])):int(shape[1]-(ratio_3*shape[1]))]\n",
    "imageio.imwrite(filepath, interp_img_crop, format='.png', compress_level=0)\n",
    "filename = 'latent_image_crop_1.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "latent_image_crop = latent_image[int(latent_shape[0]-(ratio_0*latent_shape[0])):int(latent_shape[0]-(ratio_1*latent_shape[0])),\n",
    "                             int(latent_shape[1]-(ratio_2*latent_shape[1])):int(latent_shape[1]-(ratio_3*latent_shape[1]))]\n",
    "imageio.imwrite(filepath, latent_image_crop, format='.png', compress_level=0)\n",
    "filename = 'full_img_crop_1.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "full_samp_img_crop = full_samp_img[int(shape[0]-(ratio_0*shape[0])):int(shape[0]-(ratio_1*shape[0])),\n",
    "                                   int(shape[1]-(ratio_2*shape[1])):int(shape[1]-(ratio_3*shape[1]))]\n",
    "imageio.imwrite(filepath, full_samp_img_crop, format='.png', compress_level=0)\n",
    "\n",
    "figsize = (7,7)\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax.imshow(full_samp_img_crop, cmap = 'gray')\n",
    "plt.title('Original Fully-Sampled Image Crop 1')\n",
    "plt.show()\n",
    "\n",
    "figsize = (7,7)\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax.imshow(latent_image_crop, cmap = 'gray')\n",
    "plt.title('Latent Image Crop 1')\n",
    "plt.show()\n",
    "\n",
    "figsize = (7,7)\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax.imshow(deep_image_crop, cmap = 'gray')\n",
    "plt.title('DL Reconstruction Crop 1')\n",
    "plt.show()\n",
    "\n",
    "figsize = (7,7)\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax.imshow(interp_img_crop, cmap = 'gray')\n",
    "plt.title('Bicubic Interpolation Crop 1')\n",
    "plt.show()\n",
    "\n",
    "# Showing Vessel Profiles\n",
    "\n",
    "DISP_CONV_FACTOR = 5 # converts pixels to micron\n",
    "\n",
    "angle = -20\n",
    "interp_crop = interp_img_crop[:interp_img_crop.shape[0]//3, \n",
    "                              :interp_img_crop.shape[1]//3]\n",
    "interp_crop = skimage.transform.rotate(interp_crop, angle)\n",
    "dl_crop = deep_image_crop[:deep_image_crop.shape[0]//3, \n",
    "                          :deep_image_crop.shape[1]//3]\n",
    "dl_crop = skimage.transform.rotate(dl_crop, angle)\n",
    "full_crop = full_samp_img_crop[:full_samp_img_crop.shape[0]//3, \n",
    "                               :full_samp_img_crop.shape[1]//3]\n",
    "full_crop = skimage.transform.rotate(full_crop, angle)\n",
    "\n",
    "figsize = (7,7)\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0.14,0.14,0.8,0.8])\n",
    "y_shape = full_crop.shape[0]\n",
    "x_shape = full_crop.shape[1]\n",
    "ax.plot(np.arange(0, 40, 1)*DISP_CONV_FACTOR, full_crop[55, 22:62], \n",
    "        np.arange(0, 40, 1)*DISP_CONV_FACTOR, interp_crop[55, 22:62],\n",
    "        np.arange(0, 40, 1)*DISP_CONV_FACTOR, dl_crop[55, 22:62])\n",
    "ax.legend(['Ground Truth', 'Bicubic', 'Deep Learning'], prop={'size': 15})\n",
    "plt.xlabel('Distance ($\\mu$m)', fontsize=18)\n",
    "plt.ylabel('Normalized PA Amplitude', fontsize=18)\n",
    "plt.xticks(fontsize = 14)\n",
    "plt.yticks(fontsize = 14)\n",
    "plt.grid()\n",
    "filename = 'vessel_compare_1.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "plt.savefig(filepath, dpi=300)\n",
    "plt.show()\n",
    "\n",
    "angle = -40\n",
    "interp_crop = interp_img_crop[:interp_img_crop.shape[0]//3, \n",
    "                              2*interp_img_crop.shape[1]//3:]\n",
    "interp_crop = skimage.transform.rotate(interp_crop, angle)\n",
    "dl_crop = deep_image_crop[:deep_image_crop.shape[0]//3, \n",
    "                          2*deep_image_crop.shape[1]//3:]\n",
    "dl_crop = skimage.transform.rotate(dl_crop, angle)\n",
    "full_crop = full_samp_img_crop[:full_samp_img_crop.shape[0]//3, \n",
    "                               2*full_samp_img_crop.shape[1]//3:]\n",
    "full_crop = skimage.transform.rotate(full_crop, angle)\n",
    "figsize = (7,7)\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0.14,0.14,0.8,0.8])\n",
    "y_shape = full_crop.shape[0]\n",
    "x_shape = full_crop.shape[1]\n",
    "ax.plot(np.arange(0, 30, 1)*DISP_CONV_FACTOR, full_crop[67, 22:52], \n",
    "        np.arange(0, 30, 1)*DISP_CONV_FACTOR, interp_crop[67, 22:52],\n",
    "        np.arange(0, 30, 1)*DISP_CONV_FACTOR, dl_crop[67, 22:52])\n",
    "ax.legend(['Ground Truth', 'Bicubic', 'Deep Learning'], prop={'size': 15})\n",
    "plt.xlabel('Distance ($\\mu$m)', fontsize=18)\n",
    "plt.ylabel('Normalized PA Amplitude', fontsize=18)\n",
    "plt.xticks(fontsize = 14)\n",
    "plt.yticks(fontsize = 14)\n",
    "plt.grid()\n",
    "filename = 'vessel_compare_2.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "plt.savefig(filepath, dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "IMAGE 2:\n",
    "'''\n",
    "ratio_0 = 1025/1280\n",
    "ratio_1 = 720/1280\n",
    "ratio_2 = 1405/2048\n",
    "ratio_3 = 1100/2048\n",
    "\n",
    "filename = 'deep_image_crop_2.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "shape = full_samp_img.shape\n",
    "latent_shape = latent_image.shape\n",
    "deep_image_crop = deep_image[int(shape[0]-(ratio_0*shape[0])):int(shape[0]-(ratio_1*shape[0])),\n",
    "                             int(shape[1]-(ratio_2*shape[1])):int(shape[1]-(ratio_3*shape[1]))]\n",
    "imageio.imwrite(filepath, deep_image_crop, format='.png', compress_level=0)\n",
    "filename = 'interp_img_crop_2.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "interp_img_crop = interp_img[int(shape[0]-(ratio_0*shape[0])):int(shape[0]-(ratio_1*shape[0])),\n",
    "                             int(shape[1]-(ratio_2*shape[1])):int(shape[1]-(ratio_3*shape[1]))]\n",
    "imageio.imwrite(filepath, interp_img_crop, format='.png', compress_level=0)\n",
    "filename = 'latent_image_crop_2.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "latent_image_crop = latent_image[int(latent_shape[0]-(ratio_0*latent_shape[0])):int(latent_shape[0]-(ratio_1*latent_shape[0])),\n",
    "                             int(latent_shape[1]-(ratio_2*latent_shape[1])):int(latent_shape[1]-(ratio_3*latent_shape[1]))]\n",
    "imageio.imwrite(filepath, latent_image_crop, format='.png', compress_level=0)\n",
    "filename = 'full_img_crop_2.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "full_samp_img_crop = full_samp_img[int(shape[0]-(ratio_0*shape[0])):int(shape[0]-(ratio_1*shape[0])),\n",
    "                                   int(shape[1]-(ratio_2*shape[1])):int(shape[1]-(ratio_3*shape[1]))]\n",
    "imageio.imwrite(filepath, full_samp_img_crop, format='.png', compress_level=0)\n",
    "\n",
    "figsize = (7,7)\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax.imshow(full_samp_img_crop, cmap = 'gray')\n",
    "plt.title('Original Fully-Sampled Image Crop 2')\n",
    "plt.show()\n",
    "\n",
    "figsize = (7,7)\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax.imshow(deep_image_crop, cmap = 'gray')\n",
    "plt.title('DL Reconstruction Crop 2')\n",
    "plt.show()\n",
    "\n",
    "figsize = (7,7)\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax.imshow(interp_img_crop, cmap = 'gray')\n",
    "plt.title('Bicubic Interpolation Crop 2')\n",
    "plt.show()\n",
    "\n",
    "# Showing Vessel Profiles\n",
    "\n",
    "DISP_CONV_FACTOR = 5 # converts pixels to micron\n",
    "\n",
    "figsize = (7,7)\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "y_shape = full_samp_img_crop.shape[0]\n",
    "x_shape = full_samp_img_crop.shape[1]\n",
    "ax.plot(np.arange(0, y_shape, 1)*DISP_CONV_FACTOR, full_samp_img_crop[:, x_shape//2], \n",
    "        np.arange(0, y_shape, 1)*DISP_CONV_FACTOR, interp_img_crop[:, x_shape//2],\n",
    "        np.arange(0, y_shape, 1)*DISP_CONV_FACTOR, deep_image_crop[:, x_shape//2])\n",
    "ax.legend(['Ground Truth', 'Bicubic', 'Deep Learning'])\n",
    "#plt.title('Vessel Profile Comparison 3')\n",
    "plt.show()\n",
    "\n",
    "figsize = (7,7)\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "y_shape = full_samp_img_crop.shape[0]\n",
    "x_shape = full_samp_img_crop.shape[1]\n",
    "ax.plot(np.arange(0, y_shape, 1)*DISP_CONV_FACTOR, full_samp_img_crop[y_shape//3, :], \n",
    "        np.arange(0, y_shape, 1)*DISP_CONV_FACTOR, interp_img_crop[y_shape//3, :],\n",
    "        np.arange(0, y_shape, 1)*DISP_CONV_FACTOR, deep_image_crop[y_shape//3, :])\n",
    "ax.legend(['Ground Truth', 'Bicubic', 'Deep Learning'])\n",
    "#plt.title('Vessel Profile Comparison 4')\n",
    "plt.xlabel('Distance ($\\mu$m)', fontsize=18)\n",
    "plt.ylabel('Normalized PA Amplitude', fontsize=18)\n",
    "plt.xticks(fontsize = 14)\n",
    "plt.yticks(fontsize = 14)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "IMAGE 3:\n",
    "'''\n",
    "ratio_0 = 525/1280\n",
    "ratio_1 = 220/1280\n",
    "ratio_2 = 1405/2048\n",
    "ratio_3 = 1100/2048\n",
    "\n",
    "filename = 'deep_image_crop_3.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "shape = full_samp_img_pad.shape\n",
    "latent_shape = latent_image_pad.shape\n",
    "deep_image_crop = deep_image[int(shape[0]-(ratio_0*shape[0])):int(shape[0]-(ratio_1*shape[0])),\n",
    "                             int(shape[1]-(ratio_2*shape[1])):int(shape[1]-(ratio_3*shape[1]))]\n",
    "imageio.imwrite(filepath, deep_image_crop, format='.png', compress_level=0)\n",
    "filename = 'interp_img_crop_3.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "interp_img_crop = interp_img[int(shape[0]-(ratio_0*shape[0])):int(shape[0]-(ratio_1*shape[0])),\n",
    "                             int(shape[1]-(ratio_2*shape[1])):int(shape[1]-(ratio_3*shape[1]))]\n",
    "imageio.imwrite(filepath, interp_img_crop, format='.png', compress_level=0)\n",
    "filename = 'latent_image_crop_3.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "latent_image_crop = latent_image[int(latent_shape[0]-(ratio_0*latent_shape[0])):int(latent_shape[0]-(ratio_1*latent_shape[0])),\n",
    "                             int(latent_shape[1]-(ratio_2*latent_shape[1])):int(latent_shape[1]-(ratio_3*latent_shape[1]))]\n",
    "imageio.imwrite(filepath, latent_image_crop, format='.png', compress_level=0)\n",
    "filename = 'full_img_crop_3.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "full_samp_img_crop = full_samp_img[int(shape[0]-(ratio_0*shape[0])):int(shape[0]-(ratio_1*shape[0])),\n",
    "                                   int(shape[1]-(ratio_2*shape[1])):int(shape[1]-(ratio_3*shape[1]))]\n",
    "imageio.imwrite(filepath, full_samp_img_crop, format='.png', compress_level=0)\n",
    "\n",
    "figsize = (7,7)\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax.imshow(full_samp_img_crop, cmap = 'gray')\n",
    "plt.title('Original Fully-Sampled Image Crop 3')\n",
    "plt.show()\n",
    "\n",
    "figsize = (7,7)\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax.imshow(deep_image_crop, cmap = 'gray')\n",
    "plt.title('DL Reconstruction Crop 3')\n",
    "plt.show()\n",
    "\n",
    "figsize = (7,7)\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax.imshow(interp_img_crop, cmap = 'gray')\n",
    "plt.title('Bicubic Interpolation Crop 3')\n",
    "plt.show()\n",
    "\n",
    "# Showing Vessel Profiles\n",
    "\n",
    "DISP_CONV_FACTOR = 5 # converts pixels to micron\n",
    "\n",
    "figsize = (7,7)\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "y_shape = full_samp_img_crop.shape[0]\n",
    "x_shape = full_samp_img_crop.shape[1]\n",
    "ax.plot(np.arange(0, y_shape, 1)*DISP_CONV_FACTOR, full_samp_img_crop[:, x_shape//2], \n",
    "        np.arange(0, y_shape, 1)*DISP_CONV_FACTOR, interp_img_crop[:, x_shape//2],\n",
    "        np.arange(0, y_shape, 1)*DISP_CONV_FACTOR, deep_image_crop[:, x_shape//2])\n",
    "ax.legend(['Ground Truth', 'Bicubic', 'Deep Learning'])\n",
    "#plt.title('Vessel Profile Comparison 5')\n",
    "plt.xlabel('Distance ($\\mu$m)', fontsize=18)\n",
    "plt.ylabel('Normalized PA Amplitude', fontsize=18)\n",
    "plt.xticks(fontsize = 14)\n",
    "plt.yticks(fontsize = 14)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "figsize = (7,7)\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "y_shape = full_samp_img_crop.shape[0]\n",
    "x_shape = full_samp_img_crop.shape[1]\n",
    "ax.plot(np.arange(0, y_shape, 1)*DISP_CONV_FACTOR, full_samp_img_crop[y_shape//3, :], \n",
    "        np.arange(0, y_shape, 1)*DISP_CONV_FACTOR, interp_img_crop[y_shape//3, :],\n",
    "        np.arange(0, y_shape, 1)*DISP_CONV_FACTOR, deep_image_crop[y_shape//3, :])\n",
    "ax.legend(['Ground Truth', 'Bicubic', 'Deep Learning'])\n",
    "#plt.title('Vessel Profile Comparison 6')\n",
    "plt.xlabel('Distance ($\\mu$m)', fontsize=18)\n",
    "plt.ylabel('Normalized PA Amplitude', fontsize=18)\n",
    "plt.xticks(fontsize = 14)\n",
    "plt.yticks(fontsize = 14)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "IMAGE 4:\n",
    "'''\n",
    "ratio_0 = 625/1280\n",
    "ratio_1 = 320/1280\n",
    "ratio_2 = 935/2048\n",
    "ratio_3 = 630/2048\n",
    "\n",
    "filename = 'deep_image_crop_4.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "shape = full_samp_img_pad.shape\n",
    "latent_shape = latent_image_pad.shape\n",
    "deep_image_crop = deep_image[int(shape[0]-(ratio_0*shape[0])):int(shape[0]-(ratio_1*shape[0])),\n",
    "                             int(shape[1]-(ratio_2*shape[1])):int(shape[1]-(ratio_3*shape[1]))]\n",
    "imageio.imwrite(filepath, deep_image_crop, format='.png', compress_level=0)\n",
    "filename = 'interp_img_crop_4.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "interp_img_crop = interp_img[int(shape[0]-(ratio_0*shape[0])):int(shape[0]-(ratio_1*shape[0])),\n",
    "                             int(shape[1]-(ratio_2*shape[1])):int(shape[1]-(ratio_3*shape[1]))]\n",
    "imageio.imwrite(filepath, interp_img_crop, format='.png', compress_level=0)\n",
    "filename = 'latent_image_crop_4.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "latent_image_crop = latent_image[int(latent_shape[0]-(ratio_0*latent_shape[0])):int(latent_shape[0]-(ratio_1*latent_shape[0])),\n",
    "                             int(latent_shape[1]-(ratio_2*latent_shape[1])):int(latent_shape[1]-(ratio_3*latent_shape[1]))]\n",
    "imageio.imwrite(filepath, latent_image_crop, format='.png', compress_level=0)\n",
    "filename = 'full_img_crop_4.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "full_samp_img_crop = full_samp_img[int(shape[0]-(ratio_0*shape[0])):int(shape[0]-(ratio_1*shape[0])),\n",
    "                                   int(shape[1]-(ratio_2*shape[1])):int(shape[1]-(ratio_3*shape[1]))]\n",
    "imageio.imwrite(filepath, full_samp_img_crop, format='.png', compress_level=0)\n",
    "\n",
    "figsize = (7,7)\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax.imshow(full_samp_img_crop, cmap = 'gray')\n",
    "plt.title('Original Fully-Sampled Image Crop 4')\n",
    "plt.show()\n",
    "\n",
    "figsize = (7,7)\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax.imshow(deep_image_crop, cmap = 'gray')\n",
    "plt.title('DL Reconstruction Crop 4')\n",
    "plt.show()\n",
    "\n",
    "figsize = (7,7)\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')\n",
    "ax.imshow(interp_img_crop, cmap = 'gray')\n",
    "plt.title('Bicubic Interpolation Crop 4')\n",
    "plt.show()\n",
    "\n",
    "# Showing Vessel Profiles\n",
    "\n",
    "DISP_CONV_FACTOR = 5 # converts pixels to micron\n",
    "\n",
    "figsize = (7,7)\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "y_shape = full_samp_img_crop.shape[0]\n",
    "x_shape = full_samp_img_crop.shape[1]\n",
    "ax.plot(np.arange(0, y_shape, 1)*DISP_CONV_FACTOR, full_samp_img_crop[:, x_shape//2], \n",
    "        np.arange(0, y_shape, 1)*DISP_CONV_FACTOR, interp_img_crop[:, x_shape//2],\n",
    "        np.arange(0, y_shape, 1)*DISP_CONV_FACTOR, deep_image_crop[:, x_shape//2])\n",
    "ax.legend(['Ground Truth', 'Bicubic', 'Deep Learning'])\n",
    "#plt.title('Vessel Profile Comparison 7')\n",
    "plt.xlabel('Distance ($\\mu$m)', fontsize=18)\n",
    "plt.ylabel('Normalized PA Amplitude', fontsize=18)\n",
    "plt.xticks(fontsize = 14)\n",
    "plt.yticks(fontsize = 14)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "figsize = (7,7)\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "y_shape = full_samp_img_crop.shape[0]\n",
    "x_shape = full_samp_img_crop.shape[1]\n",
    "ax.plot(np.arange(0, y_shape, 1)*DISP_CONV_FACTOR, full_samp_img_crop[y_shape//3, :], \n",
    "        np.arange(0, y_shape, 1)*DISP_CONV_FACTOR, interp_img_crop[y_shape//3, :],\n",
    "        np.arange(0, y_shape, 1)*DISP_CONV_FACTOR, deep_image_crop[y_shape//3, :])\n",
    "ax.legend(['Ground Truth', 'Bicubic', 'Deep Learning'])\n",
    "#plt.title('Vessel Profile Comparison 8')\n",
    "plt.xlabel('Distance ($\\mu$m)', fontsize=18)\n",
    "plt.ylabel('Normalized PA Amplitude', fontsize=18)\n",
    "plt.xticks(fontsize = 14)\n",
    "plt.yticks(fontsize = 14)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Statistical Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantitative Measurements\n",
    "from utils.model_utils import PSNR\n",
    "from utils.model_utils import SSIM\n",
    "from utils.model_utils import KLDivergence\n",
    "deep_image = deep_image[..., None]\n",
    "deep_image = tf.image.convert_image_dtype(deep_image[None, ...], tf.float32)\n",
    "interp_img = interp_img[..., None]\n",
    "interp_img = tf.image.convert_image_dtype(interp_img[None, ...], tf.float32)\n",
    "full_samp_img = full_samp_img[..., None]\n",
    "full_samp_img = tf.image.convert_image_dtype(full_samp_img[None, ...], tf.float32)\n",
    "print('---------------------------------------')\n",
    "print('##########LIST OF STATISTICS:##########')\n",
    "print('---------------------------------------')\n",
    "print('COMPARISON OF PSNR:')\n",
    "print('Deep Learning: ' + str(PSNR(full_samp_img, deep_image).numpy()[0]))\n",
    "print('Bicubic Interp: ' + str(PSNR(full_samp_img, interp_img).numpy()[0]))\n",
    "print('COMPARISON OF SSIM:')\n",
    "print('Deep Learning: ' + str(SSIM(full_samp_img, deep_image).numpy()[0]))\n",
    "print('Bicubic Interp: ' + str(SSIM(full_samp_img, interp_img).numpy()[0]))\n",
    "print('COMPARISON OF MEAN ABSOLUTE ERROR:')\n",
    "MAE = tf.keras.losses.MeanAbsoluteError()\n",
    "print('Deep Learning: ' + str(MAE(full_samp_img, deep_image).numpy()))\n",
    "print('Bicubic Interp: ' + str(MAE(full_samp_img, interp_img).numpy()))\n",
    "print('COMPARISON OF MEAN SQUARED ERROR:')\n",
    "MSE = tf.keras.losses.MeanSquaredError()\n",
    "print('Deep Learning: ' + str(MSE(full_samp_img, deep_image).numpy()))\n",
    "print('Bicubic Interp: ' + str(MSE(full_samp_img, interp_img).numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Collecting Test Statistics for All of the Test Image Dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model_utils import model_loss\n",
    "from utils.model_utils import model_loss_experimental\n",
    "from utils.model_utils import PSNR\n",
    "from utils.model_utils import SSIM\n",
    "from utils.model_utils import KLDivergence\n",
    "from utils.model_utils import SavingMetric\n",
    "print('Loading Model...')\n",
    "input_dir = 'saved_model_FD_UNet_5i-10j\\\\best_models_from_model_dir_ssim_psnr'\n",
    "USE_MSE = True\n",
    "file_list = [file for file in os.listdir(input_dir) if os.path.isfile(os.path.join(input_dir, file))]\n",
    "filename = sorted(file_list, key = lambda x : int(x.partition('h_')[2].partition('-S')[0]))[-1]\n",
    "directory = os.path.join(os.getcwd(), input_dir)\n",
    "directory = os.path.join(directory, filename)\n",
    "fourier_loss = model_loss(B1=0.0, B2=1.0, name='fourier_loss')\n",
    "save_func = model_loss_experimental(B1=0,B2=0,B3=1, mse = USE_MSE, name='save_func')\n",
    "ssim_psnr = model_loss_experimental(B1=0,B2=0,B3=1, mse = USE_MSE, name='ssim_psnr')\n",
    "saved_model = tf.keras.models.load_model(directory, custom_objects={'model_loss':model_loss,'loss_func':model_loss(B1=0.99,B2=0.01), \n",
    "                                                                    'PSNR':PSNR, 'SSIM':SSIM, 'KLDivergence':KLDivergence, \n",
    "                                                                    'SavingMetric':SavingMetric, 'fourier_loss':fourier_loss, \n",
    "                                                                    'save_func':save_func, 'ssim_psnr':ssim_psnr})\n",
    "print('Done Loading Best Model (' + filename + ') from: ' + input_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain Test Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Statements:\n",
    "from utils.test_statistics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Test Statistics:\n",
    "model = saved_model\n",
    "input_dir = 'data\\\\test\\\\input'\n",
    "output_dir = 'output_imgs_FD_UNet_5i-10j'\n",
    "file_format = '.png'\n",
    "downsampling_ratio = (5,10)\n",
    "contrast_enhance = None#(0.01,99.99)#(1.1, 98.90)\n",
    "interp = False # If you want to use bicubic pre-reconstruction\n",
    "gauss_blur_std = None\n",
    "remove_pad = False\n",
    "latent_img_input = False#False#True\n",
    "i_ratio = 1\n",
    "j_ratio = 1\n",
    "stats, model_times, total_times, shape_list = obtain_test_stats(model, input_dir, downsampling_ratio, \n",
    "                                                                contrast_enhance = contrast_enhance, \n",
    "                                                                interp=interp, gauss_blur_std=gauss_blur_std,\n",
    "                                                                output_dir = output_dir, file_format = file_format,\n",
    "                                                                remove_pad = remove_pad, latent_img_input = latent_img_input,\n",
    "                                                                i_ratio = i_ratio, j_ratio = j_ratio)\n",
    "# Save Stats Data:\n",
    "if not latent_img_input:\n",
    "    directory = f'{output_dir}/stats_data'\n",
    "    if not os.path.exists(directory):\n",
    "        os.mkdir(directory)\n",
    "    stats_df_dl = pd.DataFrame.from_dict(stats['Deep Learning'])\n",
    "    stats_df_bc = pd.DataFrame.from_dict(stats['Bicubic Interpolation'])\n",
    "    stats_df_lanczos = pd.DataFrame.from_dict(stats['Lanczos Interpolation'])\n",
    "    stats_df_zero = pd.DataFrame.from_dict(stats['Zero Fill'])\n",
    "    stats_df_dl.to_csv(f'{directory}/stats_{downsampling_ratio[0]}i-{downsampling_ratio[1]}j_down_dl.csv', index=False)\n",
    "    stats_df_bc.to_csv(f'{directory}/stats_{downsampling_ratio[0]}i-{downsampling_ratio[1]}j_down_bc.csv', index=False)\n",
    "    stats_df_lanczos.to_csv(f'{directory}/stats_{downsampling_ratio[0]}i-{downsampling_ratio[1]}j_down_lanczos.csv', index=False)\n",
    "    stats_df_zero.to_csv(f'{directory}/stats_{downsampling_ratio[0]}i-{downsampling_ratio[1]}j_down_zero.csv', index=False)\n",
    "    print('\\nDone Saving!!!\\n')\n",
    "\n",
    "    display_stats(stats, model_times, total_times, shape_list, directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Model Performance versus Downsampling Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'stats_data'\n",
    "metrics = ['Method','Downsampling Ratio','PSNR', 'MS-SSIM','SSIM', 'MEAN ABSOLUTE ERROR', 'MEAN SQUARED ERROR']\n",
    "stats_df = pd.DataFrame(columns=metrics)\n",
    "index = 0\n",
    "for i in range(len(os.listdir(directory))):\n",
    "    filename = os.listdir(directory)[i]\n",
    "    if filename != '.ipynb_checkpoints':\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        data = pd.read_csv(filepath)\n",
    "        down_ratio = re.findall(\"\\d+\", filename)\n",
    "        down_ratio = [ele for ele in reversed(down_ratio)] \n",
    "        down_ratio = [int(j) for j in down_ratio]\n",
    "        if filename[-6:-4] == 'dl':\n",
    "            model_type = 'Deep Learning'\n",
    "        else:\n",
    "            model_type = 'Bicubic Interpolation'\n",
    "        data_df = data[data.columns[-5:]]\n",
    "        data_df.insert(0, 'Method', model_type)\n",
    "        data_df.insert(1, 'Downsampling Ratio', str(down_ratio))\n",
    "        stats_df = pd.concat([stats_df, data_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Stats for [1, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df_1_5 = stats_df.loc[stats_df['Downsampling Ratio'] == '[5, 1]']\n",
    "stats_df_1_5_dl = stats_df_1_5.loc[stats_df_1_5['Method'] == 'Deep Learning'][['PSNR', 'MS-SSIM','SSIM', 'MEAN ABSOLUTE ERROR', 'MEAN SQUARED ERROR']]\n",
    "stats_df_1_5_bc = stats_df_1_5.loc[stats_df_1_5['Method'] == 'Bicubic Interpolation'][['PSNR', 'MS-SSIM','SSIM', 'MEAN ABSOLUTE ERROR', 'MEAN SQUARED ERROR']]\n",
    "# PSNR\n",
    "metric = 'PSNR'\n",
    "print(f'\\n-----------{metric}-----------')\n",
    "mean = np.mean(stats_df_1_5_dl[metric])\n",
    "stdev = np.std(stats_df_1_5_dl[metric])\n",
    "print(f'Deep Learning (mean): {mean}')\n",
    "print(f'Deep Learning (sd): {stdev}')\n",
    "mean = np.mean(stats_df_1_5_bc[metric])\n",
    "stdev = np.std(stats_df_1_5_bc[metric])\n",
    "print(f'Bicubic Interpolation (mean): {mean}')\n",
    "print(f'Bicubic Interpolation (sd): {stdev}')\n",
    "\n",
    "# SSIM\n",
    "metric = 'SSIM'\n",
    "print(f'\\n-----------{metric}-----------')\n",
    "mean = np.mean(stats_df_1_5_dl[metric])\n",
    "stdev = np.std(stats_df_1_5_dl[metric])\n",
    "print(f'Deep Learning (mean): {mean}')\n",
    "print(f'Deep Learning (sd): {stdev}')\n",
    "mean = np.mean(stats_df_1_5_bc[metric])\n",
    "stdev = np.std(stats_df_1_5_bc[metric])\n",
    "print(f'Bicubic Interpolation (mean): {mean}')\n",
    "print(f'Bicubic Interpolation (sd): {stdev}')\n",
    "\n",
    "# MS-SSIM\n",
    "metric = 'MS-SSIM'\n",
    "print(f'\\n-----------{metric}-----------')\n",
    "mean = np.mean(stats_df_1_5_dl[metric])\n",
    "stdev = np.std(stats_df_1_5_dl[metric])\n",
    "print(f'Deep Learning (mean): {mean}')\n",
    "print(f'Deep Learning (sd): {stdev}')\n",
    "mean = np.mean(stats_df_1_5_bc[metric])\n",
    "stdev = np.std(stats_df_1_5_bc[metric])\n",
    "print(f'Bicubic Interpolation (mean): {mean}')\n",
    "print(f'Bicubic Interpolation (sd): {stdev}')\n",
    "\n",
    "# MAE\n",
    "metric = 'MEAN ABSOLUTE ERROR'\n",
    "print(f'\\n-----------{metric}-----------')\n",
    "mean = np.mean(stats_df_1_5_dl[metric])\n",
    "stdev = np.std(stats_df_1_5_dl[metric])\n",
    "print(f'Deep Learning (mean): {mean}')\n",
    "print(f'Deep Learning (sd): {stdev}')\n",
    "mean = np.mean(stats_df_1_5_bc[metric])\n",
    "stdev = np.std(stats_df_1_5_bc[metric])\n",
    "print(f'Bicubic Interpolation (mean): {mean}')\n",
    "print(f'Bicubic Interpolation (sd): {stdev}')\n",
    "\n",
    "# MSE\n",
    "metric = 'MEAN SQUARED ERROR'\n",
    "print(f'\\n-----------{metric}-----------')\n",
    "mean = np.mean(stats_df_1_5_dl[metric])\n",
    "stdev = np.std(stats_df_1_5_dl[metric])\n",
    "print(f'Deep Learning (mean): {mean}')\n",
    "print(f'Deep Learning (sd): {stdev}')\n",
    "mean = np.mean(stats_df_1_5_bc[metric])\n",
    "stdev = np.std(stats_df_1_5_bc[metric])\n",
    "print(f'Bicubic Interpolation (mean): {mean}')\n",
    "print(f'Bicubic Interpolation (sd): {stdev}')\n",
    "print('\\n-----------END-----------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Stats for [3, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df_3_7 = stats_df.loc[stats_df['Downsampling Ratio'] == '[7, 3]']\n",
    "stats_df_3_7_dl = stats_df_3_7.loc[stats_df_3_7['Method'] == 'Deep Learning'][['PSNR', 'SSIM', 'MS-SSIM', 'MEAN ABSOLUTE ERROR', 'MEAN SQUARED ERROR']]\n",
    "stats_df_3_7_bc = stats_df_3_7.loc[stats_df_3_7['Method'] == 'Bicubic Interpolation'][['PSNR', 'SSIM', 'MS-SSIM', 'MEAN ABSOLUTE ERROR', 'MEAN SQUARED ERROR']]\n",
    "# PSNR\n",
    "metric = 'PSNR'\n",
    "print(f'\\n-----------{metric}-----------')\n",
    "mean = np.mean(stats_df_3_7_dl[metric])\n",
    "stdev = np.std(stats_df_3_7_dl[metric])\n",
    "print(f'Deep Learning (mean): {mean}')\n",
    "print(f'Deep Learning (sd): {stdev}')\n",
    "mean = np.mean(stats_df_3_7_bc[metric])\n",
    "stdev = np.std(stats_df_3_7_bc[metric])\n",
    "print(f'Bicubic Interpolation (mean): {mean}')\n",
    "print(f'Bicubic Interpolation (sd): {stdev}')\n",
    "\n",
    "# SSIM\n",
    "metric = 'SSIM'\n",
    "print(f'\\n-----------{metric}-----------')\n",
    "mean = np.mean(stats_df_3_7_dl[metric])\n",
    "stdev = np.std(stats_df_3_7_dl[metric])\n",
    "print(f'Deep Learning (mean): {mean}')\n",
    "print(f'Deep Learning (sd): {stdev}')\n",
    "mean = np.mean(stats_df_3_7_bc[metric])\n",
    "stdev = np.std(stats_df_3_7_bc[metric])\n",
    "print(f'Bicubic Interpolation (mean): {mean}')\n",
    "print(f'Bicubic Interpolation (sd): {stdev}')\n",
    "\n",
    "# 'MS-SSIM'\n",
    "metric = 'MS-SSIM'\n",
    "print(f'\\n-----------{metric}-----------')\n",
    "mean = np.mean(stats_df_1_5_dl[metric])\n",
    "stdev = np.std(stats_df_1_5_dl[metric])\n",
    "print(f'Deep Learning (mean): {mean}')\n",
    "print(f'Deep Learning (sd): {stdev}')\n",
    "mean = np.mean(stats_df_1_5_bc[metric])\n",
    "stdev = np.std(stats_df_1_5_bc[metric])\n",
    "print(f'Bicubic Interpolation (mean): {mean}')\n",
    "print(f'Bicubic Interpolation (sd): {stdev}')\n",
    "\n",
    "# MAE\n",
    "metric = 'MEAN ABSOLUTE ERROR'\n",
    "print(f'\\n-----------{metric}-----------')\n",
    "mean = np.mean(stats_df_3_7_dl[metric])\n",
    "stdev = np.std(stats_df_3_7_dl[metric])\n",
    "print(f'Deep Learning (mean): {mean}')\n",
    "print(f'Deep Learning (sd): {stdev}')\n",
    "mean = np.mean(stats_df_3_7_bc[metric])\n",
    "stdev = np.std(stats_df_3_7_bc[metric])\n",
    "print(f'Bicubic Interpolation (mean): {mean}')\n",
    "print(f'Bicubic Interpolation (sd): {stdev}')\n",
    "\n",
    "# MSE\n",
    "metric = 'MEAN SQUARED ERROR'\n",
    "print(f'\\n-----------{metric}-----------')\n",
    "mean = np.mean(stats_df_3_7_dl[metric])\n",
    "stdev = np.std(stats_df_3_7_dl[metric])\n",
    "print(f'Deep Learning (mean): {mean}')\n",
    "print(f'Deep Learning (sd): {stdev}')\n",
    "mean = np.mean(stats_df_3_7_bc[metric])\n",
    "stdev = np.std(stats_df_3_7_bc[metric])\n",
    "print(f'Bicubic Interpolation (mean): {mean}')\n",
    "print(f'Bicubic Interpolation (sd): {stdev}')\n",
    "print('\\n-----------END-----------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Stats for [5, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df_5_10 = stats_df.loc[stats_df['Downsampling Ratio'] == '[10, 5]']\n",
    "stats_df_5_10_dl = stats_df_5_10.loc[stats_df_5_10['Method'] == 'Deep Learning'][['PSNR', 'SSIM', 'MS-SSIM', 'MEAN ABSOLUTE ERROR', 'MEAN SQUARED ERROR']]\n",
    "stats_df_5_10_bc = stats_df_5_10.loc[stats_df_5_10['Method'] == 'Bicubic Interpolation'][['PSNR', 'SSIM', 'MS-SSIM', 'MEAN ABSOLUTE ERROR', 'MEAN SQUARED ERROR']]\n",
    "# PSNR\n",
    "metric = 'PSNR'\n",
    "print(f'\\n-----------{metric}-----------')\n",
    "mean = np.mean(stats_df_5_10_dl[metric])\n",
    "stdev = np.std(stats_df_5_10_dl[metric])\n",
    "print(f'Deep Learning (mean): {mean}')\n",
    "print(f'Deep Learning (sd): {stdev}')\n",
    "mean = np.mean(stats_df_5_10_bc[metric])\n",
    "stdev = np.std(stats_df_5_10_bc[metric])\n",
    "print(f'Bicubic Interpolation (mean): {mean}')\n",
    "print(f'Bicubic Interpolation (sd): {stdev}')\n",
    "\n",
    "# SSIM\n",
    "metric = 'SSIM'\n",
    "print(f'\\n-----------{metric}-----------')\n",
    "mean = np.mean(stats_df_5_10_dl[metric])\n",
    "stdev = np.std(stats_df_5_10_dl[metric])\n",
    "print(f'Deep Learning (mean): {mean}')\n",
    "print(f'Deep Learning (sd): {stdev}')\n",
    "mean = np.mean(stats_df_5_10_bc[metric])\n",
    "stdev = np.std(stats_df_5_10_bc[metric])\n",
    "print(f'Bicubic Interpolation (mean): {mean}')\n",
    "print(f'Bicubic Interpolation (sd): {stdev}')\n",
    "\n",
    "# 'MS-SSIM'\n",
    "metric = 'MS-SSIM'\n",
    "print(f'\\n-----------{metric}-----------')\n",
    "mean = np.mean(stats_df_1_5_dl[metric])\n",
    "stdev = np.std(stats_df_1_5_dl[metric])\n",
    "print(f'Deep Learning (mean): {mean}')\n",
    "print(f'Deep Learning (sd): {stdev}')\n",
    "mean = np.mean(stats_df_1_5_bc[metric])\n",
    "stdev = np.std(stats_df_1_5_bc[metric])\n",
    "print(f'Bicubic Interpolation (mean): {mean}')\n",
    "print(f'Bicubic Interpolation (sd): {stdev}')\n",
    "\n",
    "# MAE\n",
    "metric = 'MEAN ABSOLUTE ERROR'\n",
    "print(f'\\n-----------{metric}-----------')\n",
    "mean = np.mean(stats_df_5_10_dl[metric])\n",
    "stdev = np.std(stats_df_5_10_dl[metric])\n",
    "print(f'Deep Learning (mean): {mean}')\n",
    "print(f'Deep Learning (sd): {stdev}')\n",
    "mean = np.mean(stats_df_5_10_bc[metric])\n",
    "stdev = np.std(stats_df_5_10_bc[metric])\n",
    "print(f'Bicubic Interpolation (mean): {mean}')\n",
    "print(f'Bicubic Interpolation (sd): {stdev}')\n",
    "\n",
    "# MSE\n",
    "metric = 'MEAN SQUARED ERROR'\n",
    "print(f'\\n-----------{metric}-----------')\n",
    "mean = np.mean(stats_df_5_10_dl[metric])\n",
    "stdev = np.std(stats_df_5_10_dl[metric])\n",
    "print(f'Deep Learning (mean): {mean}')\n",
    "print(f'Deep Learning (sd): {stdev}')\n",
    "mean = np.mean(stats_df_5_10_bc[metric])\n",
    "stdev = np.std(stats_df_5_10_bc[metric])\n",
    "print(f'Bicubic Interpolation (mean): {mean}')\n",
    "print(f'Bicubic Interpolation (sd): {stdev}')\n",
    "print('\\n-----------END-----------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Figure Comparing Model Performance vs. Downsampling Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'stats_compare_figs'\n",
    "if not os.path.exists(directory):\n",
    "    os.mkdir(directory)\n",
    "scale = 3.2\n",
    "figsize = (15,10)\n",
    "LEGEND_SIZE = 29\n",
    "# PSNR Plot\n",
    "sns.set_style(style='white')\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0.14,0.14,0.8,0.8])\n",
    "sns.set(font_scale=scale) \n",
    "sns.boxplot(x='Downsampling Ratio', y='PSNR', hue='Method', \n",
    "            data=stats_df.loc[:,['Method', 'Downsampling Ratio', 'PSNR']], \n",
    "            order=['[5, 1]', '[7, 3]', '[10, 5]'], palette='Set1')\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles=handles[0:], labels=labels[0:], fontsize=LEGEND_SIZE, facecolor='white')\n",
    "plt.title('PSNR vs. Downsampling Ratio')\n",
    "plt.grid(axis='y')\n",
    "filename = 'PSNR.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "plt.savefig(filepath, dpi=400)\n",
    "plt.show()\n",
    "\n",
    "# SSIM Plot\n",
    "sns.set_style(style='white')\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0.14,0.14,0.8,0.8])\n",
    "sns.set(font_scale=scale)\n",
    "sns.boxplot(x='Downsampling Ratio', y='SSIM', hue='Method', \n",
    "            data=stats_df.loc[:,['Method', 'Downsampling Ratio', 'SSIM']], \n",
    "            order=['[5, 1]', '[7, 3]', '[10, 5]'], palette='Set1')\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles=handles[0:], labels=labels[0:], fontsize=LEGEND_SIZE, facecolor='white')\n",
    "plt.title('SSIM vs. Downsampling Ratio')\n",
    "plt.grid(axis='y')\n",
    "filename = 'SSIM.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "plt.savefig(filepath, dpi=400)\n",
    "plt.show()\n",
    "\n",
    "# MS-SSIM Plot\n",
    "sns.set_style(style='white')\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0.14,0.14,0.8,0.8])\n",
    "sns.set(font_scale=scale)\n",
    "sns.boxplot(x='Downsampling Ratio', y='MS-SSIM', hue='Method', \n",
    "            data=stats_df.loc[:,['Method', 'Downsampling Ratio', 'MS-SSIM']], \n",
    "            order=['[5, 1]', '[7, 3]', '[10, 5]'], palette='Set1')\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles=handles[0:], labels=labels[0:], fontsize=LEGEND_SIZE, facecolor='white')\n",
    "plt.title('MS-SSIM vs. Downsampling Ratio')\n",
    "plt.grid(axis='y')\n",
    "filename = 'MS-SSIM.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "plt.savefig(filepath, dpi=400)\n",
    "plt.show()\n",
    "\n",
    "# Mean Absolute Error Plot\n",
    "sns.set_style(style='white')\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0.14,0.14,0.8,0.8])\n",
    "sns.set(font_scale=scale) \n",
    "sns.boxplot(x='Downsampling Ratio', y='MEAN ABSOLUTE ERROR', hue='Method', \n",
    "            data=stats_df.loc[:,['Method', 'Downsampling Ratio', 'MEAN ABSOLUTE ERROR']], \n",
    "            order=['[5, 1]', '[7, 3]', '[10, 5]'], palette='Set1')\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles=handles[0:], labels=labels[0:], fontsize=LEGEND_SIZE, facecolor='white')\n",
    "plt.title('Mean Absolute Error vs. Downsampling Ratio')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.grid(axis='y')\n",
    "filename = 'MAE.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "plt.savefig(filepath, dpi=400)\n",
    "plt.show()\n",
    "\n",
    "# Mean Squared Error Plot\n",
    "sns.set_style(style='white')\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_axes([0.14,0.14,0.8,0.8])\n",
    "sns.set(font_scale=scale) \n",
    "sns.boxplot(x='Downsampling Ratio', y='MEAN SQUARED ERROR', hue='Method', \n",
    "            data=stats_df.loc[:,['Method', 'Downsampling Ratio', 'MEAN SQUARED ERROR']], \n",
    "            order=['[5, 1]', '[7, 3]', '[10, 5]'], palette='Set1')\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles=handles[0:], labels=labels[0:], fontsize=LEGEND_SIZE, facecolor='white')\n",
    "plt.title('Mean Squared Error vs. Downsampling Ratio')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.grid(axis='y')\n",
    "filename = 'MSE.png'\n",
    "filepath = os.path.join(directory, filename)\n",
    "plt.savefig(filepath, dpi=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BME590L-ML_Imaging-Final_Project_Code.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc-showtags": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
